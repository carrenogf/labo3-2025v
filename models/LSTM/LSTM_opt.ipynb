{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f76caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carre\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de la matriz: (780, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "study_name = \"lstm_opt_2019\"\n",
    "\n",
    "df = pd.read_csv(\"../../datasets/sell-in.txt.gz\", sep=\"\\t\")\n",
    "\n",
    "productos_ok = pd.read_csv(\"https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/product_id_apredecir201912.txt\", sep=\"\\t\")\n",
    "df = df[df[\"product_id\"].isin(productos_ok[\"product_id\"])]\n",
    "df = df.groupby(by=[\"periodo\",\"product_id\"]).agg({\"tn\":\"sum\"}).reset_index()\n",
    "df[\"periodo\"] = pd.to_datetime(df[\"periodo\"], format=\"%Y%m\")\n",
    "df_pivot = df.pivot(index=\"product_id\", columns=\"periodo\", values=\"tn\").fillna(0)\n",
    "print(f\"Shape de la matriz: {df_pivot.shape}\")  # (800 productos x 36 meses)\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c3b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}  # Guardamos los scalers para invertir luego\n",
    "scaled_data = np.zeros_like(df_pivot.values)\n",
    "\n",
    "for i, (producto, fila) in enumerate(df_pivot.iterrows()):\n",
    "    scaler = RobustScaler()\n",
    "    scaled_data[i] = scaler.fit_transform(fila.values.reshape(-1, 1)).flatten()\n",
    "    scalers[producto] = scaler  # Almacenar scaler para este producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3483d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, periodos_target = [], [], []\n",
    "\n",
    "for i_producto, producto in enumerate(df_pivot.index):\n",
    "    serie = scaled_data[i_producto]\n",
    "    for i in range(12, len(serie) - 2):  # Asegurar que t+2 exista\n",
    "\n",
    "        y.append(serie[i+2])  # t+2 (i+1 es el offset desde el fin de la ventana)\n",
    "        periodos_target.append(df_pivot.columns[i + 2])\n",
    "        per = df_pivot.columns[i]  # el último mes dentro de la ventana\n",
    "        mes = pd.to_datetime(per, format=\"%Y%m\").month\n",
    "        quarter = pd.to_datetime(per, format=\"%Y%m\").quarter\n",
    "        \n",
    "        mes_scaled = (mes - 1) / 11  # Normalizado a [0, 1]\n",
    "        quarter_scaled = (quarter - 1) / 3  # quarter ∈ [0, 3]\n",
    "        \n",
    "        X.append(\n",
    "            np.concatenate([\n",
    "            serie[i-12:i].reshape(-1, 1),  # Shape: (12, 1)\n",
    "            np.array([[mes_scaled], [quarter_scaled]])  # Shape: (2, 1)\n",
    "            ], axis=0)\n",
    "        )  # Shape final: (14, 1)\n",
    "        \n",
    "\n",
    "X = np.array(X)  # Shape: (n_muestras, 14, 1)\n",
    "y = np.array(y).reshape(-1, 1)  # Shape: (n_muestras, 1)\n",
    "periodos = np.array(periodos_target)\n",
    "# Ahora sí, aplicá las máscaras\n",
    "train_mask = (periodos != pd.to_datetime(\"2019-02-01\")) & (periodos <= pd.to_datetime(\"2019-12-31\"))\n",
    "test_mask = periodos == pd.to_datetime(\"2019-02-01\")\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7184e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 14:57:04,620] A new study created in RDB with name: lstm_opt_2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 258ms/step - loss: 88.8228 - mae: 1.4810 - val_loss: 1.8982 - val_mae: 0.8767\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 85.9731 - mae: 1.5582 - val_loss: 1.7581 - val_mae: 0.9309\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 83.6576 - mae: 1.5699 - val_loss: 1.1849 - val_mae: 0.8159\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 80.6840 - mae: 1.5896 - val_loss: 2.3772 - val_mae: 1.2126\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 82.4047 - mae: 1.6361 - val_loss: 1.7329 - val_mae: 0.5591\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 80.1088 - mae: 1.6416 - val_loss: 1.3236 - val_mae: 0.7417\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 81.4699 - mae: 1.6862 - val_loss: 0.9958 - val_mae: 0.6189\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 78.3724 - mae: 1.7044 - val_loss: 2.1832 - val_mae: 0.9635\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 79.8752 - mae: 1.7392 - val_loss: 2.0850 - val_mae: 1.0240\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 80.1412 - mae: 1.7652 - val_loss: 0.9805 - val_mae: 0.6542\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 77.9650 - mae: 1.7121 - val_loss: 1.1035 - val_mae: 0.7680\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 79.6806 - mae: 1.7584 - val_loss: 4.0741 - val_mae: 1.7836\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 79.7732 - mae: 1.7943 - val_loss: 2.1622 - val_mae: 1.0104\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 79.7306 - mae: 1.7282 - val_loss: 1.4876 - val_mae: 0.7982\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 76.2166 - mae: 1.6752 - val_loss: 2.0821 - val_mae: 1.2504\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 76.6122 - mae: 1.7372 - val_loss: 1.5294 - val_mae: 0.9027\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 78.3612 - mae: 1.7288 - val_loss: 0.9222 - val_mae: 0.6049\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 76.8882 - mae: 1.7194 - val_loss: 6.3977 - val_mae: 2.2193\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 81.3237 - mae: 1.7580 - val_loss: 4.7950 - val_mae: 1.7980\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 82.0189 - mae: 1.7237 - val_loss: 1.1991 - val_mae: 0.6528\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 30s 237ms/step - loss: 78.9540 - mae: 1.7038 - val_loss: 6.7219 - val_mae: 2.3141\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 78.1058 - mae: 1.6732 - val_loss: 2.1969 - val_mae: 1.2307\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 78.9815 - mae: 1.7306 - val_loss: 1.2657 - val_mae: 0.6496\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 76.6363 - mae: 1.7223 - val_loss: 3.1763 - val_mae: 1.2021\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 80.7464 - mae: 1.7192 - val_loss: 1.7778 - val_mae: 0.8127\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 77.8242 - mae: 1.7138 - val_loss: 1.2053 - val_mae: 0.7250\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 32s 246ms/step - loss: 90.9523 - mae: 1.7619 - val_loss: 1.3218 - val_mae: 0.8551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 15:11:37,493] Trial 0 finished with value: 0.9222404956817627 and parameters: {'l1': 0.0005772229507253608, 'l2': 2.9119285595315917e-05, 'lstm_units_1': 32, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.12303749939045538, 'dropout_rate': 0.41345078620888676, 'lstm_units_2': 16, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.06322661072271746, 'dense_units': 32, 'activation_dense': 'linear', 'optimizer': 'rmsprop', 'learning_rate': 0.0062212907616810914}. Best is trial 0 with value: 0.9222404956817627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 262ms/step - loss: 94.0245 - mae: 1.2729 - val_loss: 0.7210 - val_mae: 0.4294\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 93.7433 - mae: 1.2760 - val_loss: 0.6959 - val_mae: 0.4529\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 92.8612 - mae: 1.2874 - val_loss: 0.7067 - val_mae: 0.5189\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 92.0954 - mae: 1.3253 - val_loss: 0.7723 - val_mae: 0.6018\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 91.1257 - mae: 1.3610 - val_loss: 0.8958 - val_mae: 0.6925\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 89.5307 - mae: 1.4120 - val_loss: 1.1074 - val_mae: 0.8066\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 34s 262ms/step - loss: 87.3926 - mae: 1.4703 - val_loss: 1.8617 - val_mae: 1.0693\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 84.2294 - mae: 1.5452 - val_loss: 2.3836 - val_mae: 1.1225\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 83.6124 - mae: 1.5196 - val_loss: 3.3366 - val_mae: 1.2595\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 82.0845 - mae: 1.4974 - val_loss: 3.8464 - val_mae: 1.2953\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 80.5237 - mae: 1.4465 - val_loss: 4.9263 - val_mae: 1.3988\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 82.8922 - mae: 1.4697 - val_loss: 3.9245 - val_mae: 1.2271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 15:18:20,576] Trial 1 finished with value: 0.6959306001663208 and parameters: {'l1': 4.39532161797545e-06, 'l2': 0.0037128161814358298, 'lstm_units_1': 128, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.3999584193540931, 'dropout_rate': 0.29257334891140624, 'lstm_units_2': 16, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.41689762903071614, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'rmsprop', 'learning_rate': 0.00010189611547405376}. Best is trial 1 with value: 0.6959306001663208.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 253ms/step - loss: 82.7032 - mae: 1.4468 - val_loss: 4.0153 - val_mae: 1.4249\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 78.5330 - mae: 1.4641 - val_loss: 2.9956 - val_mae: 1.3095\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 76.3148 - mae: 1.5079 - val_loss: 1.9078 - val_mae: 0.7978\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 79.3291 - mae: 1.5274 - val_loss: 2.0112 - val_mae: 0.7572\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 74.0528 - mae: 1.5079 - val_loss: 1.9473 - val_mae: 0.6529\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.0407 - mae: 1.5651 - val_loss: 2.1640 - val_mae: 0.6313\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 73.7425 - mae: 1.4659 - val_loss: 3.4304 - val_mae: 1.1156\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 75.2949 - mae: 1.5116 - val_loss: 3.9588 - val_mae: 0.9747\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 75.4537 - mae: 1.5177 - val_loss: 6.4385 - val_mae: 1.0156\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 80.2606 - mae: 1.6118 - val_loss: 6.3464 - val_mae: 1.2369\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 75.5207 - mae: 1.6137 - val_loss: 3.9634 - val_mae: 1.1401\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 79.5474 - mae: 1.5682 - val_loss: 4.2492 - val_mae: 1.0395\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 74.8615 - mae: 1.5775 - val_loss: 2.6903 - val_mae: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 15:25:28,263] Trial 2 finished with value: 1.9077965021133423 and parameters: {'l1': 2.147312250631739e-05, 'l2': 0.0038289611580715637, 'lstm_units_1': 128, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.05749847833075178, 'dropout_rate': 0.39125490616590175, 'lstm_units_2': 64, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.46623624240189343, 'dense_units': 32, 'activation_dense': 'linear', 'optimizer': 'adam', 'learning_rate': 0.0027912388761031116}. Best is trial 1 with value: 0.6959306001663208.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 261ms/step - loss: 24452.6133 - mae: 2.9007 - val_loss: 0.8515 - val_mae: 0.7817\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 78.0821 - mae: 1.3828 - val_loss: 0.9974 - val_mae: 0.7956\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 95.7777 - mae: 1.4558 - val_loss: 1.8109 - val_mae: 0.8830\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 68.9557 - mae: 1.3917 - val_loss: 2.4302 - val_mae: 0.9704\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 79.6367 - mae: 1.4327 - val_loss: 3.2379 - val_mae: 1.0292\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 34s 264ms/step - loss: 88.9431 - mae: 1.4234 - val_loss: 2.2502 - val_mae: 0.9451\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 83.5798 - mae: 1.4264 - val_loss: 3.1116 - val_mae: 1.1419\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 127.5006 - mae: 1.4837 - val_loss: 1.5549 - val_mae: 0.8359\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 130.1078 - mae: 1.5022 - val_loss: 1.1546 - val_mae: 0.7473\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 84.7925 - mae: 1.4265 - val_loss: 3.6920 - val_mae: 1.0314\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 88.6858 - mae: 1.4536 - val_loss: 1.4358 - val_mae: 0.7913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 15:31:39,168] Trial 3 finished with value: 0.8514943718910217 and parameters: {'l1': 1.2681478174465453e-05, 'l2': 1.8478696367082793e-06, 'lstm_units_1': 128, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.10963530553426526, 'dropout_rate': 0.2676627213894479, 'lstm_units_2': 16, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.30628602541113087, 'dense_units': 16, 'activation_dense': 'linear', 'optimizer': 'rmsprop', 'learning_rate': 0.004469475543643752}. Best is trial 1 with value: 0.6959306001663208.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 264ms/step - loss: 92.3881 - mae: 1.3220 - val_loss: 0.5683 - val_mae: 0.6260\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 89.8861 - mae: 1.4192 - val_loss: 1.4337 - val_mae: 0.8518\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 86.9779 - mae: 1.4534 - val_loss: 3.1098 - val_mae: 1.0676\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 84.8647 - mae: 1.4697 - val_loss: 3.0371 - val_mae: 1.0351\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 83.6469 - mae: 1.4597 - val_loss: 4.6563 - val_mae: 1.0403\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 80.2214 - mae: 1.4397 - val_loss: 3.7045 - val_mae: 0.9905\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 78.6994 - mae: 1.4449 - val_loss: 4.8513 - val_mae: 1.1372\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 77.4869 - mae: 1.4453 - val_loss: 3.4469 - val_mae: 1.0795\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 77.7142 - mae: 1.4123 - val_loss: 4.3740 - val_mae: 1.2045\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 74.5839 - mae: 1.4414 - val_loss: 2.7231 - val_mae: 1.0147\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 72.6916 - mae: 1.4478 - val_loss: 1.7339 - val_mae: 0.8199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 15:37:49,195] Trial 4 finished with value: 0.5683001279830933 and parameters: {'l1': 1.495846966532625e-06, 'l2': 3.810458956915754e-05, 'lstm_units_1': 128, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.22339956048956477, 'dropout_rate': 0.2880462237079822, 'lstm_units_2': 16, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.3495245893661933, 'dense_units': 32, 'activation_dense': 'relu', 'optimizer': 'rmsprop', 'learning_rate': 0.00028921379150513576}. Best is trial 4 with value: 0.5683001279830933.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 257ms/step - loss: 88.4226 - mae: 1.4846 - val_loss: 1.8418 - val_mae: 0.9352\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 87.9761 - mae: 1.5812 - val_loss: 1.3083 - val_mae: 0.5666\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 88.0250 - mae: 1.5547 - val_loss: 0.7702 - val_mae: 0.6029\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 87.4340 - mae: 1.5926 - val_loss: 1.9585 - val_mae: 0.7723\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 87.5677 - mae: 1.6344 - val_loss: 0.8379 - val_mae: 0.5806\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 86.8570 - mae: 1.6101 - val_loss: 2.0188 - val_mae: 0.7640\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 87.9142 - mae: 1.7076 - val_loss: 1.2243 - val_mae: 0.7433\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 87.2459 - mae: 1.6371 - val_loss: 2.0498 - val_mae: 1.1783\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 87.1492 - mae: 1.6741 - val_loss: 0.8626 - val_mae: 0.5729\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 88.0392 - mae: 1.5766 - val_loss: 2.1010 - val_mae: 1.1723\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 88.1811 - mae: 1.7446 - val_loss: 1.2430 - val_mae: 0.7205\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 87.7667 - mae: 1.6576 - val_loss: 1.3511 - val_mae: 0.6230\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 89.1756 - mae: 1.7550 - val_loss: 1.6131 - val_mae: 0.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 15:44:54,707] Trial 5 finished with value: 0.7702248096466064 and parameters: {'l1': 5.168131072582368e-08, 'l2': 1.156936771255692e-05, 'lstm_units_1': 64, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.25186390115019575, 'dropout_rate': 0.2423355149067902, 'lstm_units_2': 32, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.17135605781756852, 'dense_units': 8, 'activation_dense': 'linear', 'optimizer': 'adam', 'learning_rate': 0.00894229465976125}. Best is trial 4 with value: 0.5683001279830933.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 260ms/step - loss: 89.2622 - mae: 1.3858 - val_loss: 1.7011 - val_mae: 0.7824\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 85.7449 - mae: 1.3855 - val_loss: 4.9191 - val_mae: 1.2142\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 83.5942 - mae: 1.4132 - val_loss: 1.2469 - val_mae: 0.7718\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 81.9351 - mae: 1.4073 - val_loss: 3.4368 - val_mae: 1.1382\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 79.9771 - mae: 1.4162 - val_loss: 3.3182 - val_mae: 1.1069\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 78.6397 - mae: 1.4126 - val_loss: 1.0929 - val_mae: 0.6536\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 77.3334 - mae: 1.3980 - val_loss: 2.0576 - val_mae: 0.8611\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 76.4530 - mae: 1.3993 - val_loss: 2.7915 - val_mae: 1.0198\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 75.8624 - mae: 1.3843 - val_loss: 2.4614 - val_mae: 1.0410\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 74.6218 - mae: 1.3852 - val_loss: 1.0141 - val_mae: 0.6187\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 73.8825 - mae: 1.3797 - val_loss: 1.8119 - val_mae: 0.9031\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 73.6698 - mae: 1.3769 - val_loss: 1.2493 - val_mae: 0.6251\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.4783 - mae: 1.3577 - val_loss: 1.2667 - val_mae: 0.6333\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 73.8048 - mae: 1.3667 - val_loss: 0.6445 - val_mae: 0.4949\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 73.1529 - mae: 1.3571 - val_loss: 1.0971 - val_mae: 0.6939\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 73.2216 - mae: 1.3370 - val_loss: 2.3989 - val_mae: 0.9420\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 72.5600 - mae: 1.3443 - val_loss: 3.1001 - val_mae: 1.0505\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 72.4817 - mae: 1.3679 - val_loss: 1.0469 - val_mae: 0.6393\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.0228 - mae: 1.3553 - val_loss: 1.2013 - val_mae: 0.6679\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.7166 - mae: 1.3493 - val_loss: 1.2251 - val_mae: 0.6925\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.6234 - mae: 1.3411 - val_loss: 1.8120 - val_mae: 0.8621\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 72.6725 - mae: 1.3342 - val_loss: 1.8511 - val_mae: 0.8266\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 71.8921 - mae: 1.3562 - val_loss: 1.6012 - val_mae: 0.7373\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 72.0619 - mae: 1.3395 - val_loss: 1.7966 - val_mae: 0.8298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 15:58:06,768] Trial 6 finished with value: 0.64454585313797 and parameters: {'l1': 0.00017648997164534645, 'l2': 0.00014224951709255722, 'lstm_units_1': 128, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.3483070064129973, 'dropout_rate': 0.41893920436779286, 'lstm_units_2': 16, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.3745966294722201, 'dense_units': 8, 'activation_dense': 'linear', 'optimizer': 'rmsprop', 'learning_rate': 0.001986392732888861}. Best is trial 4 with value: 0.5683001279830933.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 40s 270ms/step - loss: 81.1201 - mae: 1.4831 - val_loss: 5.5481 - val_mae: 1.2410\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 75.6439 - mae: 1.5328 - val_loss: 5.2550 - val_mae: 1.1898\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 73.8166 - mae: 1.4535 - val_loss: 4.8974 - val_mae: 1.1895\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 73.9092 - mae: 1.4281 - val_loss: 5.4191 - val_mae: 1.2981\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.5781 - mae: 1.4499 - val_loss: 2.1631 - val_mae: 0.9341\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 74.5346 - mae: 1.3994 - val_loss: 2.4969 - val_mae: 0.9222\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 72.2171 - mae: 1.3765 - val_loss: 6.6824 - val_mae: 1.3397\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 73.9778 - mae: 1.4023 - val_loss: 3.8671 - val_mae: 1.0861\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 74.8935 - mae: 1.3855 - val_loss: 1.8503 - val_mae: 0.8553\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 72.3831 - mae: 1.3597 - val_loss: 3.8791 - val_mae: 1.1801\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 71.5320 - mae: 1.3841 - val_loss: 4.6002 - val_mae: 1.2338\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 70.2673 - mae: 1.3961 - val_loss: 2.0576 - val_mae: 0.9348\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 71.6510 - mae: 1.3647 - val_loss: 4.3066 - val_mae: 1.2138\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 72.4465 - mae: 1.3830 - val_loss: 2.7757 - val_mae: 1.0073\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 34s 265ms/step - loss: 71.5447 - mae: 1.3804 - val_loss: 2.9011 - val_mae: 1.0700\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 69.1597 - mae: 1.3844 - val_loss: 2.8732 - val_mae: 1.0277\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 70.4952 - mae: 1.3977 - val_loss: 2.1123 - val_mae: 0.9085\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 72.6100 - mae: 1.3472 - val_loss: 4.3614 - val_mae: 1.2163\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 73.1947 - mae: 1.3667 - val_loss: 5.3407 - val_mae: 1.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:08:43,792] Trial 7 finished with value: 1.8502776622772217 and parameters: {'l1': 1.1667864412971933e-07, 'l2': 1.5915444720862628e-08, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.15380330584007207, 'dropout_rate': 0.22924653319163502, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.3771051912655053, 'dense_units': 32, 'activation_dense': 'linear', 'optimizer': 'rmsprop', 'learning_rate': 0.0008822977701289018}. Best is trial 4 with value: 0.5683001279830933.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 258ms/step - loss: 93.1675 - mae: 1.2703 - val_loss: 0.4894 - val_mae: 0.4538\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 92.7549 - mae: 1.2930 - val_loss: 0.5867 - val_mae: 0.5754\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 92.1969 - mae: 1.3871 - val_loss: 0.7601 - val_mae: 0.7060\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 91.3026 - mae: 1.4829 - val_loss: 0.9345 - val_mae: 0.8149\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 90.4292 - mae: 1.5404 - val_loss: 1.4972 - val_mae: 1.0746\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 88.7330 - mae: 1.6730 - val_loss: 1.8737 - val_mae: 1.1750\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 87.2477 - mae: 1.6873 - val_loss: 1.7169 - val_mae: 1.0742\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 84.7843 - mae: 1.7079 - val_loss: 3.6377 - val_mae: 1.4779\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 84.7431 - mae: 1.7102 - val_loss: 3.4203 - val_mae: 1.3770\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 82.2433 - mae: 1.7757 - val_loss: 3.6453 - val_mae: 1.3180\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 81.8250 - mae: 1.7421 - val_loss: 1.6929 - val_mae: 0.8179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:14:46,119] Trial 8 finished with value: 0.489362508058548 and parameters: {'l1': 2.094529066125968e-07, 'l2': 0.0014592516985924165, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.0886364933609679, 'dropout_rate': 0.4930100380001061, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.22596095393193127, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00011861757787711701}. Best is trial 8 with value: 0.489362508058548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 264ms/step - loss: 78.3538 - mae: 1.4037 - val_loss: 1.9514 - val_mae: 1.0293\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 78.7865 - mae: 1.4523 - val_loss: 4.0407 - val_mae: 1.2303\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 84.9184 - mae: 1.4890 - val_loss: 1.3256 - val_mae: 0.7488\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 97.3705 - mae: 1.4618 - val_loss: 3.3338 - val_mae: 0.9766\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 82.4431 - mae: 1.4579 - val_loss: 0.9317 - val_mae: 0.6663\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 81.3792 - mae: 1.4731 - val_loss: 2.0182 - val_mae: 1.0742\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 87.1597 - mae: 1.5009 - val_loss: 0.4998 - val_mae: 0.5009\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 83.2484 - mae: 1.4823 - val_loss: 2.7762 - val_mae: 1.1407\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 76.4619 - mae: 1.5062 - val_loss: 2.0569 - val_mae: 0.9405\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 91.6223 - mae: 1.5125 - val_loss: 1.7022 - val_mae: 0.7847\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 84.4333 - mae: 1.4709 - val_loss: 2.3504 - val_mae: 1.0149\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 86.1376 - mae: 1.5373 - val_loss: 2.4677 - val_mae: 1.0629\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 83.2906 - mae: 1.4989 - val_loss: 2.9654 - val_mae: 1.4552\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 86.7868 - mae: 1.5226 - val_loss: 1.4107 - val_mae: 0.8280\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 104.0967 - mae: 1.5477 - val_loss: 2.1799 - val_mae: 0.9247\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 90.7708 - mae: 1.5458 - val_loss: 1.7089 - val_mae: 0.9485\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 91.1457 - mae: 1.6332 - val_loss: 1.3108 - val_mae: 1.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:24:12,571] Trial 9 finished with value: 0.4998036026954651 and parameters: {'l1': 5.006759138376855e-06, 'l2': 0.0005594109007829361, 'lstm_units_1': 64, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.447605123622229, 'dropout_rate': 0.27412414837551846, 'lstm_units_2': 16, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.2501428210063551, 'dense_units': 16, 'activation_dense': 'linear', 'optimizer': 'rmsprop', 'learning_rate': 0.008502835153397514}. Best is trial 8 with value: 0.489362508058548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 260ms/step - loss: 93.0830 - mae: 1.2686 - val_loss: 0.3969 - val_mae: 0.4626\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 92.3722 - mae: 1.3210 - val_loss: 0.5541 - val_mae: 0.6213\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 91.2480 - mae: 1.4270 - val_loss: 0.7621 - val_mae: 0.7614\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 89.0077 - mae: 1.4878 - val_loss: 1.5974 - val_mae: 1.1309\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 86.4899 - mae: 1.7626 - val_loss: 1.4659 - val_mae: 1.0302\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 34s 267ms/step - loss: 84.1133 - mae: 1.7720 - val_loss: 1.2673 - val_mae: 0.8966\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 81.4802 - mae: 1.6339 - val_loss: 4.6967 - val_mae: 1.5601\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 79.8456 - mae: 1.6620 - val_loss: 3.5520 - val_mae: 1.2629\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 79.3767 - mae: 1.6414 - val_loss: 3.0866 - val_mae: 1.1167\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 78.9846 - mae: 1.5744 - val_loss: 4.3392 - val_mae: 1.2384\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 78.7066 - mae: 1.5708 - val_loss: 3.3891 - val_mae: 1.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:30:16,568] Trial 10 finished with value: 0.3968925178050995 and parameters: {'l1': 1.1391397960879903e-08, 'l2': 6.193536638365469e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.01039922234338475, 'dropout_rate': 0.11696484217031405, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.13869472158348425, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00014761610367510509}. Best is trial 10 with value: 0.3968925178050995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 257ms/step - loss: 93.1412 - mae: 1.2700 - val_loss: 0.3914 - val_mae: 0.4400\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 92.8183 - mae: 1.2861 - val_loss: 0.5085 - val_mae: 0.5819\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 92.2335 - mae: 1.3889 - val_loss: 0.6298 - val_mae: 0.6813\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 91.0994 - mae: 1.4781 - val_loss: 1.1988 - val_mae: 0.9698\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 89.2049 - mae: 1.5967 - val_loss: 2.9596 - val_mae: 1.4278\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 86.4969 - mae: 1.7124 - val_loss: 5.7225 - val_mae: 1.6466\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 84.3438 - mae: 1.7018 - val_loss: 8.4043 - val_mae: 1.7805\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 81.6326 - mae: 1.6648 - val_loss: 3.5686 - val_mae: 1.1654\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 80.8475 - mae: 1.6210 - val_loss: 6.7741 - val_mae: 1.5243\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 81.2500 - mae: 1.6322 - val_loss: 2.8778 - val_mae: 1.0042\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 79.3925 - mae: 1.5515 - val_loss: 9.1340 - val_mae: 1.7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:36:20,635] Trial 11 finished with value: 0.39141586422920227 and parameters: {'l1': 1.449497828981721e-08, 'l2': 4.878161418123229e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.006030067449175447, 'dropout_rate': 0.10536510168142041, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.13357661165613974, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00011141690931622219}. Best is trial 11 with value: 0.39141586422920227.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 256ms/step - loss: 92.9893 - mae: 1.2819 - val_loss: 0.5996 - val_mae: 0.6442\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 90.7269 - mae: 1.4986 - val_loss: 3.4197 - val_mae: 1.6294\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 85.4337 - mae: 1.7022 - val_loss: 2.5903 - val_mae: 1.1583\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 82.3829 - mae: 1.7097 - val_loss: 2.4951 - val_mae: 1.0275\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 81.0060 - mae: 1.5088 - val_loss: 5.8455 - val_mae: 1.5331\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 77.1572 - mae: 1.6100 - val_loss: 1.6898 - val_mae: 0.8976\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 77.2440 - mae: 1.5142 - val_loss: 6.3891 - val_mae: 1.6595\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 74.9261 - mae: 1.6712 - val_loss: 3.7648 - val_mae: 1.1526\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 78.1099 - mae: 1.5630 - val_loss: 4.4863 - val_mae: 1.2724\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 75.0623 - mae: 1.5820 - val_loss: 5.0719 - val_mae: 1.2678\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 73.5622 - mae: 1.5363 - val_loss: 9.4882 - val_mae: 1.5779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:42:23,322] Trial 12 finished with value: 0.5995814204216003 and parameters: {'l1': 1.107559299727199e-08, 'l2': 2.635196021436288e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.0002958045258377954, 'dropout_rate': 0.10722578240207589, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.07675111159939552, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0003080378081229479}. Best is trial 11 with value: 0.39141586422920227.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 92.5061 - mae: 1.3345 - val_loss: 0.7369 - val_mae: 0.7441\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 88.5683 - mae: 1.4890 - val_loss: 1.8099 - val_mae: 1.0034\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 81.0468 - mae: 1.6123 - val_loss: 1.1716 - val_mae: 0.7002\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.8573 - mae: 1.5009 - val_loss: 4.1074 - val_mae: 1.1394\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 74.3725 - mae: 1.4815 - val_loss: 1.5701 - val_mae: 0.7189\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 74.7776 - mae: 1.4431 - val_loss: 3.5322 - val_mae: 0.9986\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 75.2706 - mae: 1.4443 - val_loss: 7.2926 - val_mae: 1.5341\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 71.8713 - mae: 1.4928 - val_loss: 5.6546 - val_mae: 1.2095\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 72.3407 - mae: 1.4526 - val_loss: 6.7328 - val_mae: 1.3763\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 71.9040 - mae: 1.5009 - val_loss: 3.6027 - val_mae: 1.0276\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 69.9352 - mae: 1.4063 - val_loss: 15.9668 - val_mae: 2.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:48:29,681] Trial 13 finished with value: 0.7368601560592651 and parameters: {'l1': 1.1640618579837215e-08, 'l2': 4.919130829966422e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.006760915564627639, 'dropout_rate': 0.12380508745557627, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.13344204595518613, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00028748973119838904}. Best is trial 11 with value: 0.39141586422920227.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 92.3053 - mae: 1.3667 - val_loss: 0.7741 - val_mae: 0.7501\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 87.0498 - mae: 1.5852 - val_loss: 3.8358 - val_mae: 1.2984\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 31s 239ms/step - loss: 79.4356 - mae: 1.5991 - val_loss: 29.6036 - val_mae: 2.8126\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 72.8246 - mae: 1.5251 - val_loss: 1.3946 - val_mae: 0.6968\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 90.3046 - mae: 1.2781 - val_loss: 2.2403 - val_mae: 0.8510\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 77.8797 - mae: 1.4678 - val_loss: 4.6781 - val_mae: 1.1922\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 73.1151 - mae: 1.4736 - val_loss: 1.9849 - val_mae: 0.8905\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 71.9777 - mae: 1.4641 - val_loss: 1.7740 - val_mae: 0.8880\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 72.0924 - mae: 1.4226 - val_loss: 3.9441 - val_mae: 1.1510\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.9720 - mae: 1.4088 - val_loss: 12.9716 - val_mae: 1.7485\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 71.7982 - mae: 1.4450 - val_loss: 4.4648 - val_mae: 1.2494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 16:54:33,430] Trial 14 finished with value: 0.7740554213523865 and parameters: {'l1': 5.732785615271793e-07, 'l2': 3.862429688783225e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.20309322975350974, 'dropout_rate': 0.177190489243933, 'lstm_units_2': 64, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.012460899996593916, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0006098267063468504}. Best is trial 11 with value: 0.39141586422920227.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 93.3009 - mae: 1.2723 - val_loss: 0.3970 - val_mae: 0.4257\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 93.1094 - mae: 1.2720 - val_loss: 0.3966 - val_mae: 0.4480\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 92.5737 - mae: 1.3098 - val_loss: 0.5743 - val_mae: 0.6391\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 90.7567 - mae: 1.4718 - val_loss: 0.9664 - val_mae: 0.8669\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 89.1017 - mae: 1.4929 - val_loss: 1.3695 - val_mae: 0.9684\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 86.6916 - mae: 1.6122 - val_loss: 2.5901 - val_mae: 1.3170\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 85.6743 - mae: 1.6224 - val_loss: 2.3758 - val_mae: 1.0390\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 83.2335 - mae: 1.6192 - val_loss: 2.7887 - val_mae: 1.0496\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 82.7351 - mae: 1.6705 - val_loss: 3.1911 - val_mae: 1.2455\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 81.4865 - mae: 1.6907 - val_loss: 4.4112 - val_mae: 1.4387\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 80.6620 - mae: 1.6015 - val_loss: 4.3925 - val_mae: 1.1393\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 80.3693 - mae: 1.5729 - val_loss: 9.7467 - val_mae: 2.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:01:11,552] Trial 15 finished with value: 0.3966349959373474 and parameters: {'l1': 3.136732732979479e-08, 'l2': 1.3188711666993009e-06, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.2795015874586646, 'dropout_rate': 0.16768155061381268, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.15132984128948018, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00016263853005806154}. Best is trial 11 with value: 0.39141586422920227.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 257ms/step - loss: 93.0625 - mae: 1.2790 - val_loss: 0.4595 - val_mae: 0.5359\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 92.0040 - mae: 1.4491 - val_loss: 0.9798 - val_mae: 0.8840\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 90.0277 - mae: 1.6215 - val_loss: 1.1485 - val_mae: 0.9421\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 85.4605 - mae: 1.6360 - val_loss: 1.6803 - val_mae: 1.0130\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 82.2329 - mae: 1.6402 - val_loss: 4.0436 - val_mae: 1.2991\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 78.2789 - mae: 1.5653 - val_loss: 1.5165 - val_mae: 0.8229\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 77.6500 - mae: 1.5570 - val_loss: 5.4568 - val_mae: 1.4394\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 78.5044 - mae: 1.5392 - val_loss: 2.5679 - val_mae: 1.0414\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 75.4748 - mae: 1.5546 - val_loss: 4.3662 - val_mae: 1.2888\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 76.3249 - mae: 1.5580 - val_loss: 2.7007 - val_mae: 1.0118\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 73.0334 - mae: 1.5326 - val_loss: 9.9182 - val_mae: 1.6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:07:17,605] Trial 16 finished with value: 0.4595387279987335 and parameters: {'l1': 4.734207069360643e-08, 'l2': 3.1162895139210485e-06, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.31382488403628817, 'dropout_rate': 0.17243072868269063, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.18923142837518026, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00019889316656526403}. Best is trial 11 with value: 0.39141586422920227.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 264ms/step - loss: 92.5665 - mae: 1.3284 - val_loss: 0.8321 - val_mae: 0.8048\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 86.2989 - mae: 1.6254 - val_loss: 6.7611 - val_mae: 1.7084\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 81.9216 - mae: 1.6983 - val_loss: 4.1647 - val_mae: 1.2966\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 80.8242 - mae: 1.5736 - val_loss: 5.0607 - val_mae: 1.7310\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 78.6801 - mae: 1.5636 - val_loss: 3.5609 - val_mae: 1.2073\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 73.6595 - mae: 1.5365 - val_loss: 4.1427 - val_mae: 1.3002\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 72.6639 - mae: 1.5368 - val_loss: 0.3405 - val_mae: 0.4445\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 79.1829 - mae: 1.3331 - val_loss: 3.3859 - val_mae: 1.2241\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.2909 - mae: 1.4713 - val_loss: 5.5435 - val_mae: 1.3779\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 75.5376 - mae: 1.4942 - val_loss: 4.5345 - val_mae: 1.2733\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 76.5282 - mae: 1.4972 - val_loss: 0.5008 - val_mae: 0.5635\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 74.6578 - mae: 1.4425 - val_loss: 3.0437 - val_mae: 1.0629\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 72.4562 - mae: 1.4426 - val_loss: 4.0731 - val_mae: 1.0131\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 74.8971 - mae: 1.4085 - val_loss: 3.4666 - val_mae: 1.1655\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.6737 - mae: 1.4840 - val_loss: 0.9703 - val_mae: 0.6706\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 69.9190 - mae: 1.4456 - val_loss: 5.6878 - val_mae: 1.2779\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 72.0411 - mae: 1.4364 - val_loss: 1.9558 - val_mae: 0.9203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:16:39,032] Trial 17 finished with value: 0.34048137068748474 and parameters: {'l1': 3.3692472030315245e-07, 'l2': 8.7876011262569e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.26612587287797906, 'dropout_rate': 0.18703491200890998, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.10895054417461136, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0005130986400377099}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 259ms/step - loss: 91.6880 - mae: 1.4037 - val_loss: 0.7850 - val_mae: 0.6706\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 88.4234 - mae: 1.4930 - val_loss: 1.4283 - val_mae: 0.7780\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 86.7128 - mae: 1.4512 - val_loss: 2.2750 - val_mae: 1.0072\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 85.5512 - mae: 1.4802 - val_loss: 1.2181 - val_mae: 0.6646\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 84.9294 - mae: 1.4777 - val_loss: 2.0059 - val_mae: 0.8148\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 83.8388 - mae: 1.4506 - val_loss: 4.7016 - val_mae: 1.1692\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 83.1598 - mae: 1.4958 - val_loss: 3.5458 - val_mae: 1.0397\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 82.4758 - mae: 1.4638 - val_loss: 2.6922 - val_mae: 0.9780\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 81.7077 - mae: 1.4558 - val_loss: 4.1614 - val_mae: 1.1341\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 80.9270 - mae: 1.4626 - val_loss: 4.2354 - val_mae: 1.0790\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 80.3944 - mae: 1.4464 - val_loss: 2.7940 - val_mae: 0.9877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:22:44,506] Trial 18 finished with value: 0.7850282192230225 and parameters: {'l1': 4.490441874359947e-07, 'l2': 8.146932205434467e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.4930785423617451, 'dropout_rate': 0.20911588835427147, 'lstm_units_2': 64, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.08729070907547705, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0006078594729819895}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 89.0049 - mae: 1.5416 - val_loss: 0.8429 - val_mae: 0.7474\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 83.1499 - mae: 1.4531 - val_loss: 0.6145 - val_mae: 0.5973\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 76.5240 - mae: 1.4954 - val_loss: 5.9415 - val_mae: 1.3276\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 74.4276 - mae: 1.4116 - val_loss: 5.2431 - val_mae: 1.3189\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 76.2272 - mae: 1.4383 - val_loss: 1.7098 - val_mae: 0.7596\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 73.5313 - mae: 1.3850 - val_loss: 3.5597 - val_mae: 1.1748\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.1004 - mae: 1.4237 - val_loss: 0.5950 - val_mae: 0.5911\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 246ms/step - loss: 77.5252 - mae: 1.3059 - val_loss: 3.1727 - val_mae: 1.0546\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 70.9484 - mae: 1.3868 - val_loss: 1.0178 - val_mae: 0.7361\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 71.4983 - mae: 1.3749 - val_loss: 0.4644 - val_mae: 0.5432\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 74.6635 - mae: 1.2705 - val_loss: 6.1648 - val_mae: 1.1891\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 70.6759 - mae: 1.4058 - val_loss: 5.8726 - val_mae: 1.2839\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 70.0771 - mae: 1.4207 - val_loss: 2.3336 - val_mae: 0.9819\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 70.1728 - mae: 1.4035 - val_loss: 4.6626 - val_mae: 1.2752\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 72.4760 - mae: 1.3157 - val_loss: 0.9706 - val_mae: 0.7416\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 72.0525 - mae: 1.3649 - val_loss: 3.7971 - val_mae: 0.9956\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 67.5816 - mae: 1.3819 - val_loss: 1.7224 - val_mae: 0.8434\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 68.2938 - mae: 1.3405 - val_loss: 1.1839 - val_mae: 0.7411\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 70.1389 - mae: 1.4008 - val_loss: 2.5336 - val_mae: 0.9280\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 66.2544 - mae: 1.3771 - val_loss: 1.3229 - val_mae: 0.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:33:39,852] Trial 19 finished with value: 0.46440884470939636 and parameters: {'l1': 1.0744288851164229e-06, 'l2': 9.206063947180759e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.36378677753188277, 'dropout_rate': 0.1491488926645028, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.019788627021821326, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0015334527406628113}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 264ms/step - loss: 91.8106 - mae: 1.3285 - val_loss: 1.0499 - val_mae: 0.8521\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 84.9809 - mae: 1.5284 - val_loss: 3.0524 - val_mae: 0.8496\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 79.8394 - mae: 1.5038 - val_loss: 2.1597 - val_mae: 1.0061\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 77.6049 - mae: 1.5470 - val_loss: 3.0659 - val_mae: 0.8775\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 78.6463 - mae: 1.5134 - val_loss: 5.6894 - val_mae: 1.2495\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.7059 - mae: 1.4639 - val_loss: 2.4367 - val_mae: 0.8092\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 75.4742 - mae: 1.4580 - val_loss: 5.8483 - val_mae: 1.3838\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 76.2088 - mae: 1.4964 - val_loss: 2.0742 - val_mae: 0.8197\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.9829 - mae: 1.4533 - val_loss: 3.7265 - val_mae: 1.1362\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 73.1903 - mae: 1.4825 - val_loss: 3.5981 - val_mae: 1.0735\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 74.7493 - mae: 1.4434 - val_loss: 2.8035 - val_mae: 1.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:39:44,579] Trial 20 finished with value: 1.0498589277267456 and parameters: {'l1': 1.8893267277567115e-07, 'l2': 1.1904344758359062e-08, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.18783299592179645, 'dropout_rate': 0.3432825187662726, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.23476724428405513, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00043136832240943987}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 92.9111 - mae: 1.2714 - val_loss: 0.4491 - val_mae: 0.5300\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 92.1063 - mae: 1.3582 - val_loss: 0.8823 - val_mae: 0.8209\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 90.3666 - mae: 1.5233 - val_loss: 1.4348 - val_mae: 1.0161\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 86.2688 - mae: 1.5945 - val_loss: 2.3144 - val_mae: 1.1581\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 82.0168 - mae: 1.6029 - val_loss: 2.9711 - val_mae: 1.1554\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 79.5430 - mae: 1.5748 - val_loss: 6.6639 - val_mae: 1.5528\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.6114 - mae: 1.5227 - val_loss: 5.9340 - val_mae: 1.4657\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 73.8557 - mae: 1.4761 - val_loss: 4.6910 - val_mae: 1.3000\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 74.3220 - mae: 1.5256 - val_loss: 2.8947 - val_mae: 0.8769\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 77.0087 - mae: 1.4355 - val_loss: 3.9548 - val_mae: 1.1406\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 73.2077 - mae: 1.4252 - val_loss: 5.9792 - val_mae: 1.4195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:45:46,513] Trial 21 finished with value: 0.449072003364563 and parameters: {'l1': 5.071371749208955e-08, 'l2': 4.065941541123445e-06, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.2692687253358268, 'dropout_rate': 0.18148819306476097, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.11409357105330578, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00018455525121420505}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 261ms/step - loss: 92.9665 - mae: 1.2760 - val_loss: 0.4665 - val_mae: 0.5479\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 92.0093 - mae: 1.3844 - val_loss: 0.7769 - val_mae: 0.7693\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 89.8246 - mae: 1.4915 - val_loss: 1.4999 - val_mae: 1.0186\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 85.7797 - mae: 1.6049 - val_loss: 1.4414 - val_mae: 0.9011\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 80.6161 - mae: 1.5778 - val_loss: 3.0220 - val_mae: 1.1363\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 79.2370 - mae: 1.5516 - val_loss: 4.1505 - val_mae: 1.1934\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 77.5902 - mae: 1.5079 - val_loss: 9.2942 - val_mae: 1.8351\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 76.1676 - mae: 1.6178 - val_loss: 5.6171 - val_mae: 1.3328\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 74.8561 - mae: 1.5477 - val_loss: 3.9129 - val_mae: 1.1101\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 75.0005 - mae: 1.5257 - val_loss: 6.8114 - val_mae: 1.4285\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.7924 - mae: 1.5115 - val_loss: 5.3270 - val_mae: 1.2548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:51:52,681] Trial 22 finished with value: 0.466476172208786 and parameters: {'l1': 3.023312614032097e-08, 'l2': 1.0574697134501914e-06, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.29476995899749536, 'dropout_rate': 0.14345513542432486, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.17820569852292775, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00019601537298064118}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 92.3600 - mae: 1.3290 - val_loss: 0.6849 - val_mae: 0.7157\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 85.7854 - mae: 1.6037 - val_loss: 1.8689 - val_mae: 1.0575\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 82.3404 - mae: 1.6612 - val_loss: 4.0678 - val_mae: 1.3110\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 77.1780 - mae: 1.5789 - val_loss: 3.4208 - val_mae: 1.1010\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 75.2633 - mae: 1.5815 - val_loss: 9.1855 - val_mae: 1.7063\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 77.6162 - mae: 1.4532 - val_loss: 11.8593 - val_mae: 1.8449\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 76.9350 - mae: 1.5195 - val_loss: 5.8774 - val_mae: 1.3526\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 75.1684 - mae: 1.4644 - val_loss: 4.3283 - val_mae: 1.2070\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 74.2500 - mae: 1.4979 - val_loss: 8.4453 - val_mae: 1.6012\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 76.7911 - mae: 1.4479 - val_loss: 9.1804 - val_mae: 1.7497\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 72.1939 - mae: 1.5052 - val_loss: 3.0365 - val_mae: 1.0083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 17:57:57,274] Trial 23 finished with value: 0.6848620176315308 and parameters: {'l1': 1.2416684234625107e-07, 'l2': 5.0368152984998084e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.1699216778293508, 'dropout_rate': 0.20806718840301647, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.2680683749937094, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00048158282794788585}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 91.5020 - mae: 1.4061 - val_loss: 2.4483 - val_mae: 1.2299\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 85.1758 - mae: 1.5105 - val_loss: 0.7342 - val_mae: 0.6559\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 78.9889 - mae: 1.4761 - val_loss: 0.4126 - val_mae: 0.4825\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 78.0057 - mae: 1.4731 - val_loss: 3.3780 - val_mae: 1.0319\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 76.1900 - mae: 1.2874 - val_loss: 0.4229 - val_mae: 0.4815\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 94.5051 - mae: 1.3178 - val_loss: 0.4502 - val_mae: 0.5170\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 92.8698 - mae: 1.3340 - val_loss: 0.4887 - val_mae: 0.5578\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 92.7926 - mae: 1.3573 - val_loss: 0.5301 - val_mae: 0.5956\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 92.7365 - mae: 1.3805 - val_loss: 0.5732 - val_mae: 0.6306\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 92.6931 - mae: 1.4052 - val_loss: 0.6208 - val_mae: 0.6659\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 92.6587 - mae: 1.4254 - val_loss: 0.6609 - val_mae: 0.6939\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 92.6324 - mae: 1.4462 - val_loss: 0.7056 - val_mae: 0.7240\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 92.6113 - mae: 1.4638 - val_loss: 0.7397 - val_mae: 0.7460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 18:05:08,376] Trial 24 finished with value: 0.412593275308609 and parameters: {'l1': 3.04863141409251e-08, 'l2': 2.3541878071650066e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.3068862263440841, 'dropout_rate': 0.15116403515123528, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.045054081153551126, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0009785560246208488}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 263ms/step - loss: 93.0979 - mae: 1.2701 - val_loss: 0.4081 - val_mae: 0.4729\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 92.6660 - mae: 1.3173 - val_loss: 0.5494 - val_mae: 0.6165\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 92.0402 - mae: 1.3964 - val_loss: 0.7744 - val_mae: 0.7690\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 90.2858 - mae: 1.5411 - val_loss: 2.9261 - val_mae: 1.3631\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 87.1349 - mae: 1.7156 - val_loss: 2.1229 - val_mae: 0.9917\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 83.8031 - mae: 1.6722 - val_loss: 13.4623 - val_mae: 2.0708\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 82.1295 - mae: 1.5951 - val_loss: 10.2762 - val_mae: 1.7935\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 80.8401 - mae: 1.5757 - val_loss: 6.7868 - val_mae: 1.4728\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 80.5013 - mae: 1.6064 - val_loss: 11.4345 - val_mae: 1.8370\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 79.4318 - mae: 1.5521 - val_loss: 4.1744 - val_mae: 1.2350\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 79.4905 - mae: 1.5406 - val_loss: 8.0117 - val_mae: 1.5939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 18:11:15,516] Trial 25 finished with value: 0.40812948346138 and parameters: {'l1': 3.4319111657090107e-07, 'l2': 2.1870307187588137e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.22785831806487697, 'dropout_rate': 0.10169300619314224, 'lstm_units_2': 64, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.110304193574068, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00010009381874601544}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 261ms/step - loss: 92.7919 - mae: 1.2815 - val_loss: 0.4446 - val_mae: 0.5356\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 92.2435 - mae: 1.3534 - val_loss: 0.6119 - val_mae: 0.6716\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 91.7779 - mae: 1.3954 - val_loss: 0.6480 - val_mae: 0.6851\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 91.0003 - mae: 1.4126 - val_loss: 0.8642 - val_mae: 0.7339\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 90.1055 - mae: 1.4667 - val_loss: 2.3569 - val_mae: 1.1199\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 89.5629 - mae: 1.5389 - val_loss: 2.4742 - val_mae: 1.0465\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 89.1546 - mae: 1.5294 - val_loss: 2.2954 - val_mae: 0.9704\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 88.6214 - mae: 1.4781 - val_loss: 2.7921 - val_mae: 1.0472\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 87.8839 - mae: 1.5175 - val_loss: 2.9448 - val_mae: 1.0544\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 87.5124 - mae: 1.5070 - val_loss: 2.6536 - val_mae: 0.9889\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 87.2117 - mae: 1.4753 - val_loss: 3.0504 - val_mae: 1.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 18:17:21,228] Trial 26 finished with value: 0.4445638060569763 and parameters: {'l1': 1.085937983531423e-07, 'l2': 7.419656767417954e-06, 'lstm_units_1': 32, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.3894042246457166, 'dropout_rate': 0.19968200228140104, 'lstm_units_2': 32, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.1955284020223148, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00014915943965813761}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 256ms/step - loss: 93.0587 - mae: 1.2744 - val_loss: 0.4447 - val_mae: 0.5171\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 92.4444 - mae: 1.3838 - val_loss: 0.7662 - val_mae: 0.7627\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 90.5022 - mae: 1.5768 - val_loss: 1.1555 - val_mae: 0.9109\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 85.8764 - mae: 1.6777 - val_loss: 2.2521 - val_mae: 1.1132\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 82.2172 - mae: 1.6944 - val_loss: 2.3335 - val_mae: 0.9893\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 246ms/step - loss: 80.7769 - mae: 1.5904 - val_loss: 7.7480 - val_mae: 1.7254\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 79.5997 - mae: 1.5732 - val_loss: 7.4168 - val_mae: 1.5803\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 81.0171 - mae: 1.5426 - val_loss: 6.1858 - val_mae: 1.4574\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 78.4941 - mae: 1.5192 - val_loss: 10.8165 - val_mae: 1.8640\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 76.4306 - mae: 1.5520 - val_loss: 6.8725 - val_mae: 1.4802\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 78.1693 - mae: 1.4139 - val_loss: 5.2243 - val_mae: 1.3877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 18:23:19,614] Trial 27 finished with value: 0.44468265771865845 and parameters: {'l1': 2.6710682709818756e-08, 'l2': 1.201634970968849e-06, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.2756547105228945, 'dropout_rate': 0.3411815118880736, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.14466300106574967, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0002522486879566643}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 264ms/step - loss: 92.6826 - mae: 1.3104 - val_loss: 0.8368 - val_mae: 0.7973\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 89.1460 - mae: 1.5374 - val_loss: 3.4166 - val_mae: 1.3829\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 80.2805 - mae: 1.5685 - val_loss: 1.7826 - val_mae: 0.7328\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 82.0572 - mae: 1.5332 - val_loss: 0.3456 - val_mae: 0.4326\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 81.1690 - mae: 1.2910 - val_loss: 1.6548 - val_mae: 0.8286\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 74.2583 - mae: 1.4371 - val_loss: 5.4300 - val_mae: 1.3599\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 74.5464 - mae: 1.5441 - val_loss: 3.5978 - val_mae: 0.9628\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 74.8861 - mae: 1.4776 - val_loss: 1.6760 - val_mae: 0.8285\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 74.2286 - mae: 1.4673 - val_loss: 4.8612 - val_mae: 1.2207\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 72.3788 - mae: 1.4442 - val_loss: 1.1251 - val_mae: 0.6871\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 77.2232 - mae: 1.2805 - val_loss: 2.5659 - val_mae: 1.0125\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 74.3850 - mae: 1.4603 - val_loss: 6.8107 - val_mae: 1.4457\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 71.4390 - mae: 1.4491 - val_loss: 4.7072 - val_mae: 1.3318\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 72.0191 - mae: 1.4957 - val_loss: 3.3418 - val_mae: 1.1574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 18:31:02,261] Trial 28 finished with value: 0.3455905318260193 and parameters: {'l1': 7.663881439085151e-08, 'l2': 2.7732744959239758e-08, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.34212204221479053, 'dropout_rate': 0.24255136333246893, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.10226292673767948, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00044233238686333903}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 259ms/step - loss: 87.9928 - mae: 1.4295 - val_loss: 3.1627 - val_mae: 0.9236\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 82.5279 - mae: 1.3999 - val_loss: 4.9925 - val_mae: 1.2785\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 78.9011 - mae: 1.4353 - val_loss: 3.1603 - val_mae: 0.9923\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.7664 - mae: 1.4136 - val_loss: 4.9789 - val_mae: 1.2465\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 75.0831 - mae: 1.4129 - val_loss: 3.0111 - val_mae: 1.0028\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 73.5514 - mae: 1.4019 - val_loss: 3.6524 - val_mae: 1.0895\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 73.8417 - mae: 1.4259 - val_loss: 3.7400 - val_mae: 1.1062\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 83.6762 - mae: 1.4630 - val_loss: 2.5034 - val_mae: 0.8879\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 77.4863 - mae: 1.4678 - val_loss: 2.8656 - val_mae: 0.9279\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 72.3978 - mae: 1.3858 - val_loss: 3.1092 - val_mae: 0.9983\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 72.4561 - mae: 1.4202 - val_loss: 3.1224 - val_mae: 0.9909\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 72.4255 - mae: 1.4285 - val_loss: 2.9679 - val_mae: 0.9605\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 71.8969 - mae: 1.4224 - val_loss: 2.2955 - val_mae: 0.8460\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 72.0504 - mae: 1.4099 - val_loss: 3.0750 - val_mae: 0.9861\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 73.4335 - mae: 1.3918 - val_loss: 4.2234 - val_mae: 1.1431\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 72.3098 - mae: 1.4296 - val_loss: 2.8620 - val_mae: 0.9382\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 71.6690 - mae: 1.3943 - val_loss: 2.2450 - val_mae: 0.8357\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 72.1830 - mae: 1.3772 - val_loss: 3.0351 - val_mae: 0.9415\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 74.2049 - mae: 1.4258 - val_loss: 2.7010 - val_mae: 0.9056\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 72.2911 - mae: 1.4370 - val_loss: 2.2848 - val_mae: 0.8014\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 72.3515 - mae: 1.4103 - val_loss: 2.2978 - val_mae: 0.8428\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 72.1370 - mae: 1.4100 - val_loss: 2.0552 - val_mae: 0.7795\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.7903 - mae: 1.3885 - val_loss: 3.1096 - val_mae: 0.9981\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 71.5293 - mae: 1.4516 - val_loss: 1.6719 - val_mae: 0.7303\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 71.8591 - mae: 1.4053 - val_loss: 2.8668 - val_mae: 0.9131\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 71.9171 - mae: 1.4145 - val_loss: 3.1856 - val_mae: 1.0051\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.1812 - mae: 1.3846 - val_loss: 2.5707 - val_mae: 0.8922\n",
      "Epoch 28/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.6593 - mae: 1.4825 - val_loss: 1.9539 - val_mae: 0.7638\n",
      "Epoch 29/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 71.3811 - mae: 1.4049 - val_loss: 2.7487 - val_mae: 0.8537\n",
      "Epoch 30/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 76.1016 - mae: 1.4653 - val_loss: 2.3203 - val_mae: 0.8109\n",
      "Epoch 31/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 71.9279 - mae: 1.3987 - val_loss: 3.2640 - val_mae: 0.9972\n",
      "Epoch 32/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 74.5004 - mae: 1.4047 - val_loss: 4.3057 - val_mae: 1.1611\n",
      "Epoch 33/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 72.7771 - mae: 1.4090 - val_loss: 3.5788 - val_mae: 1.0522\n",
      "Epoch 34/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 73.3431 - mae: 1.4098 - val_loss: 3.3571 - val_mae: 1.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 18:49:29,959] Trial 29 finished with value: 1.6719268560409546 and parameters: {'l1': 0.0008821856762440447, 'l2': 2.3370418528040863e-08, 'lstm_units_1': 64, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.3451451190464209, 'dropout_rate': 0.24306161416414, 'lstm_units_2': 64, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.052262474832731255, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0014223783979157329}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 258ms/step - loss: 90.2918 - mae: 1.4353 - val_loss: 1.8251 - val_mae: 0.8924\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 78.4771 - mae: 1.4568 - val_loss: 4.4253 - val_mae: 1.1812\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 79.8672 - mae: 1.4232 - val_loss: 1.7573 - val_mae: 0.8765\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 75.7032 - mae: 1.4869 - val_loss: 8.2775 - val_mae: 1.5499\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 74.2809 - mae: 1.4890 - val_loss: 1.6068 - val_mae: 0.8239\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 73.7405 - mae: 1.3673 - val_loss: 3.6770 - val_mae: 0.9439\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 77.4044 - mae: 1.3549 - val_loss: 0.5783 - val_mae: 0.5859\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 72.2557 - mae: 1.3877 - val_loss: 2.0606 - val_mae: 0.9505\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 69.5058 - mae: 1.4173 - val_loss: 2.6832 - val_mae: 0.9207\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 71.7459 - mae: 1.4009 - val_loss: 12.7538 - val_mae: 1.6391\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 68.7903 - mae: 1.3922 - val_loss: 1.8322 - val_mae: 0.8946\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 72.1622 - mae: 1.3814 - val_loss: 2.4153 - val_mae: 0.9565\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 70.5438 - mae: 1.3781 - val_loss: 2.3216 - val_mae: 0.9474\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 68.8243 - mae: 1.4255 - val_loss: 4.0396 - val_mae: 1.2222\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 66.0593 - mae: 1.3290 - val_loss: 1.7628 - val_mae: 0.8746\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 68.6744 - mae: 1.3062 - val_loss: 4.8356 - val_mae: 1.3384\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 70.0722 - mae: 1.4895 - val_loss: 3.0230 - val_mae: 1.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 18:58:51,652] Trial 30 finished with value: 0.5782867670059204 and parameters: {'l1': 6.561741410764736e-07, 'l2': 1.0141898447353824e-07, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.4297345918691362, 'dropout_rate': 0.31812816404271604, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.0978700536407403, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0007360936377782675}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 256ms/step - loss: 91.4777 - mae: 1.3493 - val_loss: 1.5393 - val_mae: 0.9768\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 81.1108 - mae: 1.5547 - val_loss: 2.4760 - val_mae: 0.7719\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 76.2579 - mae: 1.4836 - val_loss: 1.8672 - val_mae: 0.9092\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 75.8244 - mae: 1.4315 - val_loss: 5.9679 - val_mae: 1.2187\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.2012 - mae: 1.5174 - val_loss: 1.4657 - val_mae: 0.7155\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 74.3575 - mae: 1.3895 - val_loss: 4.9620 - val_mae: 1.1753\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 71.7357 - mae: 1.4062 - val_loss: 5.8187 - val_mae: 1.2241\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 72.1850 - mae: 1.4355 - val_loss: 6.8825 - val_mae: 1.4011\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 69.0128 - mae: 1.4051 - val_loss: 0.8966 - val_mae: 0.6190\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 71.9273 - mae: 1.3747 - val_loss: 1.7075 - val_mae: 0.8800\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 75.3479 - mae: 1.4069 - val_loss: 3.3838 - val_mae: 1.1204\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 72.7601 - mae: 1.4498 - val_loss: 4.5919 - val_mae: 1.2182\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 72.0275 - mae: 1.4131 - val_loss: 5.1658 - val_mae: 1.3234\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 68.5729 - mae: 1.4104 - val_loss: 2.9585 - val_mae: 1.0168\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 69.4504 - mae: 1.4325 - val_loss: 1.6622 - val_mae: 0.8099\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 67.7492 - mae: 1.3604 - val_loss: 2.7203 - val_mae: 1.0146\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 71.4076 - mae: 1.2923 - val_loss: 0.9871 - val_mae: 0.5407\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 77.9738 - mae: 1.1902 - val_loss: 2.6940 - val_mae: 0.6729\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 71.4417 - mae: 1.2319 - val_loss: 2.0631 - val_mae: 0.7943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:09:14,900] Trial 31 finished with value: 0.8966304063796997 and parameters: {'l1': 7.285441813323188e-08, 'l2': 1.836564207945388e-07, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.325789046375279, 'dropout_rate': 0.13904789790377622, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.15694708680206446, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00043713954474181125}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 257ms/step - loss: 92.7891 - mae: 1.2711 - val_loss: 0.4781 - val_mae: 0.5595\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 91.1334 - mae: 1.3544 - val_loss: 0.7816 - val_mae: 0.7556\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 246ms/step - loss: 87.8719 - mae: 1.5009 - val_loss: 1.4345 - val_mae: 0.8818\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 83.0558 - mae: 1.5753 - val_loss: 4.8690 - val_mae: 1.6334\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 79.1766 - mae: 1.7439 - val_loss: 2.0626 - val_mae: 0.8507\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 76.8189 - mae: 1.5796 - val_loss: 5.6008 - val_mae: 1.3440\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 74.7846 - mae: 1.4908 - val_loss: 4.1902 - val_mae: 1.2914\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 73.2710 - mae: 1.4842 - val_loss: 4.9945 - val_mae: 1.2345\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 72.0392 - mae: 1.4585 - val_loss: 3.5465 - val_mae: 1.1187\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 71.8267 - mae: 1.4426 - val_loss: 0.5721 - val_mae: 0.5263\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 71.4140 - mae: 1.4050 - val_loss: 3.2324 - val_mae: 1.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:15:18,691] Trial 32 finished with value: 0.4781339168548584 and parameters: {'l1': 1.900455255562876e-08, 'l2': 5.22642986556953e-07, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.24172300825299917, 'dropout_rate': 0.1749872587890703, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.04591130004471339, 'dense_units': 32, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00013890756412345296}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 258ms/step - loss: 92.6240 - mae: 1.3466 - val_loss: 0.7216 - val_mae: 0.7373\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 89.8503 - mae: 1.5825 - val_loss: 1.9059 - val_mae: 1.0193\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 83.1344 - mae: 1.6217 - val_loss: 2.9288 - val_mae: 1.0834\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 80.8760 - mae: 1.5770 - val_loss: 2.3994 - val_mae: 0.9991\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 77.2079 - mae: 1.5516 - val_loss: 3.8233 - val_mae: 1.1532\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 78.3896 - mae: 1.4978 - val_loss: 5.8406 - val_mae: 1.3907\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 77.2736 - mae: 1.5530 - val_loss: 6.8465 - val_mae: 1.4521\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 74.2267 - mae: 1.4840 - val_loss: 3.9112 - val_mae: 1.1529\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 74.6972 - mae: 1.4775 - val_loss: 2.6872 - val_mae: 0.9880\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.0450 - mae: 1.4834 - val_loss: 2.6092 - val_mae: 0.9583\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 74.3452 - mae: 1.4305 - val_loss: 4.1656 - val_mae: 1.1718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:21:18,640] Trial 33 finished with value: 0.7215905785560608 and parameters: {'l1': 2.798707062818382e-07, 'l2': 3.177517901834808e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.3887200982160786, 'dropout_rate': 0.22319634893118168, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.21024855689090743, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00036857291416275337}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 263ms/step - loss: 91.2563 - mae: 1.2718 - val_loss: 0.4605 - val_mae: 0.5542\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 81.9301 - mae: 1.3118 - val_loss: 0.6517 - val_mae: 0.6569\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 79.1937 - mae: 1.3492 - val_loss: 1.2360 - val_mae: 0.8476\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 84.8841 - mae: 1.4676 - val_loss: 2.0593 - val_mae: 0.9795\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 75.1145 - mae: 1.4356 - val_loss: 1.9554 - val_mae: 0.9328\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 78.5256 - mae: 1.4616 - val_loss: 3.7714 - val_mae: 1.2062\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 72.9853 - mae: 1.4399 - val_loss: 4.8577 - val_mae: 1.3179\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 81.9661 - mae: 1.4543 - val_loss: 2.8049 - val_mae: 1.0155\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 73.7193 - mae: 1.4080 - val_loss: 4.5994 - val_mae: 1.2712\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 76.0674 - mae: 1.4367 - val_loss: 2.9648 - val_mae: 1.0223\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 76.5869 - mae: 1.4388 - val_loss: 2.3063 - val_mae: 0.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:27:28,747] Trial 34 finished with value: 0.46047645807266235 and parameters: {'l1': 1.91418274991738e-06, 'l2': 1.0154405845272117e-08, 'lstm_units_1': 128, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.2660991165926247, 'dropout_rate': 0.2598861064779156, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.07973146027028684, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'rmsprop', 'learning_rate': 0.00022995230628774813}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 86.5690 - mae: 1.4504 - val_loss: 1.2290 - val_mae: 0.6633\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 77.5966 - mae: 1.3710 - val_loss: 2.1874 - val_mae: 0.9383\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 77.3217 - mae: 1.3950 - val_loss: 1.4765 - val_mae: 0.7962\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 72.8053 - mae: 1.3646 - val_loss: 2.7404 - val_mae: 0.8982\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 70.6885 - mae: 1.3816 - val_loss: 2.2325 - val_mae: 0.8855\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 75.4402 - mae: 1.4378 - val_loss: 2.5525 - val_mae: 0.9387\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 76.2881 - mae: 1.4010 - val_loss: 3.8591 - val_mae: 1.0483\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 74.4925 - mae: 1.4429 - val_loss: 2.7302 - val_mae: 1.0398\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 71.9909 - mae: 1.4350 - val_loss: 2.8736 - val_mae: 0.9903\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 70.0508 - mae: 1.4174 - val_loss: 3.3542 - val_mae: 1.1827\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 69.5847 - mae: 1.4502 - val_loss: 3.2449 - val_mae: 1.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:33:32,773] Trial 35 finished with value: 1.2289783954620361 and parameters: {'l1': 0.00010767154566579256, 'l2': 2.232613193503906e-05, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.1268650199394368, 'dropout_rate': 0.16260269563384125, 'lstm_units_2': 16, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.11684805474669202, 'dense_units': 32, 'activation_dense': 'linear', 'optimizer': 'adam', 'learning_rate': 0.0013063309300035482}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 255ms/step - loss: 88.9314 - mae: 1.3909 - val_loss: 1.7911 - val_mae: 0.7384\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 83.4140 - mae: 1.4195 - val_loss: 2.8423 - val_mae: 0.6750\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 80.4545 - mae: 1.3669 - val_loss: 2.7934 - val_mae: 0.9218\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 76.9559 - mae: 1.3826 - val_loss: 2.7130 - val_mae: 0.7841\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 246ms/step - loss: 74.9806 - mae: 1.4092 - val_loss: 2.9474 - val_mae: 0.7624\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 76.1250 - mae: 1.4155 - val_loss: 5.8946 - val_mae: 1.0647\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 72.8810 - mae: 1.4170 - val_loss: 3.4068 - val_mae: 0.8846\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 73.6067 - mae: 1.3895 - val_loss: 3.3183 - val_mae: 0.8676\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.7418 - mae: 1.4544 - val_loss: 3.0843 - val_mae: 0.8006\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 80.1455 - mae: 1.4528 - val_loss: 4.3619 - val_mae: 0.8144\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 74.9283 - mae: 1.3907 - val_loss: 4.4215 - val_mae: 0.9088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:39:34,438] Trial 36 finished with value: 1.7910840511322021 and parameters: {'l1': 7.059034221543387e-08, 'l2': 0.008591558909652476, 'lstm_units_1': 128, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.04804778868112216, 'dropout_rate': 0.19596129013153804, 'lstm_units_2': 32, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.1437372751144519, 'dense_units': 32, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0030908958871169894}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 260ms/step - loss: 86.7309 - mae: 1.2761 - val_loss: 0.4973 - val_mae: 0.5867\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 80.4919 - mae: 1.3308 - val_loss: 0.5754 - val_mae: 0.6294\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 82.4935 - mae: 1.3284 - val_loss: 0.7310 - val_mae: 0.6945\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 78.2308 - mae: 1.3567 - val_loss: 0.9243 - val_mae: 0.7282\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 76.9229 - mae: 1.3656 - val_loss: 1.3640 - val_mae: 0.8276\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 34s 266ms/step - loss: 92.4845 - mae: 1.4126 - val_loss: 1.4701 - val_mae: 0.8378\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 79.7937 - mae: 1.3940 - val_loss: 2.0691 - val_mae: 0.9404\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 75.0304 - mae: 1.4326 - val_loss: 2.3371 - val_mae: 0.9729\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 74.9455 - mae: 1.4379 - val_loss: 2.6361 - val_mae: 1.0011\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 75.1484 - mae: 1.4475 - val_loss: 2.2750 - val_mae: 0.9346\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 75.5792 - mae: 1.4309 - val_loss: 1.9947 - val_mae: 0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:45:44,455] Trial 37 finished with value: 0.49734580516815186 and parameters: {'l1': 6.392830961346084e-06, 'l2': 2.906402554658978e-06, 'lstm_units_1': 32, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.2885687009282385, 'dropout_rate': 0.13590872822456754, 'lstm_units_2': 16, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.3065564410937432, 'dense_units': 8, 'activation_dense': 'linear', 'optimizer': 'rmsprop', 'learning_rate': 0.00036141594127821175}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 255ms/step - loss: 91.4219 - mae: 1.2863 - val_loss: 0.6022 - val_mae: 0.6328\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 88.4476 - mae: 1.3717 - val_loss: 1.1865 - val_mae: 0.8098\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 84.9420 - mae: 1.5004 - val_loss: 1.7566 - val_mae: 0.8679\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 80.9208 - mae: 1.4821 - val_loss: 5.0839 - val_mae: 1.3219\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 246ms/step - loss: 75.0503 - mae: 1.5187 - val_loss: 3.5496 - val_mae: 0.9879\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 75.9468 - mae: 1.4850 - val_loss: 4.1448 - val_mae: 1.1656\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 74.8198 - mae: 1.5332 - val_loss: 4.6539 - val_mae: 1.1691\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 75.4444 - mae: 1.4770 - val_loss: 4.4781 - val_mae: 1.2410\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 75.1059 - mae: 1.4635 - val_loss: 5.5847 - val_mae: 1.3155\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 73.3134 - mae: 1.5180 - val_loss: 4.5096 - val_mae: 1.2210\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 71.4834 - mae: 1.4814 - val_loss: 5.9290 - val_mae: 1.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:51:44,152] Trial 38 finished with value: 0.6022328734397888 and parameters: {'l1': 2.137165337226606e-08, 'l2': 9.61742244878896e-07, 'lstm_units_1': 128, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.21285409319494275, 'dropout_rate': 0.3055603622910348, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.48064544678205867, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00011878085991268392}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 257ms/step - loss: 91.7163 - mae: 1.3474 - val_loss: 0.6360 - val_mae: 0.6760\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 89.8646 - mae: 1.4262 - val_loss: 0.9630 - val_mae: 0.7460\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 88.6079 - mae: 1.4404 - val_loss: 1.4071 - val_mae: 0.8444\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 87.3757 - mae: 1.4244 - val_loss: 1.6430 - val_mae: 0.8263\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 86.4382 - mae: 1.4063 - val_loss: 1.8556 - val_mae: 0.8504\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 85.5788 - mae: 1.3972 - val_loss: 2.1332 - val_mae: 0.9153\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 84.7925 - mae: 1.4050 - val_loss: 1.6365 - val_mae: 0.8091\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 84.1415 - mae: 1.3848 - val_loss: 2.3206 - val_mae: 0.9166\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 83.4604 - mae: 1.3951 - val_loss: 2.2860 - val_mae: 0.8868\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 82.8134 - mae: 1.3875 - val_loss: 2.0925 - val_mae: 0.9641\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 82.3176 - mae: 1.4206 - val_loss: 1.9527 - val_mae: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 19:57:50,053] Trial 39 finished with value: 0.6359583139419556 and parameters: {'l1': 1.025180472369221e-07, 'l2': 5.528277032402742e-08, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.33851452501209117, 'dropout_rate': 0.24703496280436937, 'lstm_units_2': 64, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.17512874145520185, 'dense_units': 16, 'activation_dense': 'linear', 'optimizer': 'rmsprop', 'learning_rate': 0.0001715758787309498}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 258ms/step - loss: 91.9492 - mae: 1.3495 - val_loss: 0.9263 - val_mae: 0.8458\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 87.4440 - mae: 1.5186 - val_loss: 0.8280 - val_mae: 0.7154\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 80.9173 - mae: 1.5266 - val_loss: 2.6890 - val_mae: 1.1396\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 77.3650 - mae: 1.4687 - val_loss: 5.1543 - val_mae: 1.3555\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 75.2790 - mae: 1.6005 - val_loss: 0.7911 - val_mae: 0.6489\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 74.0384 - mae: 1.4380 - val_loss: 11.2700 - val_mae: 1.6870\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 72.9239 - mae: 1.5007 - val_loss: 5.0893 - val_mae: 1.3191\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.1938 - mae: 1.4871 - val_loss: 3.0919 - val_mae: 1.0272\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 72.4560 - mae: 1.4594 - val_loss: 0.7499 - val_mae: 0.6231\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 70.5610 - mae: 1.4176 - val_loss: 4.9890 - val_mae: 1.2487\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 73.7553 - mae: 1.4533 - val_loss: 4.8087 - val_mae: 1.1980\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 76.3594 - mae: 1.4307 - val_loss: 4.4837 - val_mae: 1.1878\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 70.4282 - mae: 1.4937 - val_loss: 1.9918 - val_mae: 0.8809\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 70.3523 - mae: 1.4255 - val_loss: 1.9398 - val_mae: 0.9308\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 71.4933 - mae: 1.4325 - val_loss: 5.0566 - val_mae: 1.2659\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 67.8004 - mae: 1.4699 - val_loss: 2.6869 - val_mae: 0.9876\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 68.8615 - mae: 1.3932 - val_loss: 5.3176 - val_mae: 1.2592\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 70.7832 - mae: 1.4112 - val_loss: 1.8564 - val_mae: 0.8495\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 67.4045 - mae: 1.4094 - val_loss: 4.7194 - val_mae: 1.2239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:08:11,302] Trial 40 finished with value: 0.7498766779899597 and parameters: {'l1': 1.7348797600284914e-07, 'l2': 4.834175628282172e-05, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.36527769343396144, 'dropout_rate': 0.22879480293606635, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.2867398640952227, 'dense_units': 32, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0005556002241614824}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 263ms/step - loss: 93.2354 - mae: 1.2696 - val_loss: 0.3978 - val_mae: 0.4210\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 93.2109 - mae: 1.2698 - val_loss: 0.3969 - val_mae: 0.4241\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 93.1896 - mae: 1.2703 - val_loss: 0.3970 - val_mae: 0.4278\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 93.1689 - mae: 1.2709 - val_loss: 0.3959 - val_mae: 0.4305\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 93.1476 - mae: 1.2710 - val_loss: 0.3960 - val_mae: 0.4342\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 93.1253 - mae: 1.2708 - val_loss: 0.3956 - val_mae: 0.4377\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 93.1023 - mae: 1.2696 - val_loss: 0.3964 - val_mae: 0.4424\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 93.0801 - mae: 1.2684 - val_loss: 0.3985 - val_mae: 0.4476\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 93.0604 - mae: 1.2697 - val_loss: 0.3979 - val_mae: 0.4501\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 93.0395 - mae: 1.2698 - val_loss: 0.3985 - val_mae: 0.4537\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 93.0211 - mae: 1.2716 - val_loss: 0.4000 - val_mae: 0.4576\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 93.0017 - mae: 1.2725 - val_loss: 0.4030 - val_mae: 0.4631\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 92.9834 - mae: 1.2743 - val_loss: 0.4019 - val_mae: 0.4646\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 92.4399 - mae: 1.2845 - val_loss: 0.4376 - val_mae: 0.5379\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 89.4612 - mae: 1.4427 - val_loss: 1.1978 - val_mae: 0.8772\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 85.2005 - mae: 1.5915 - val_loss: 4.4495 - val_mae: 1.4681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:17:02,612] Trial 41 finished with value: 0.3956359028816223 and parameters: {'l1': 1.1451446735700757e-08, 'l2': 4.551200819632534e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.041948042627397354, 'dropout_rate': 0.11899316010655597, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.13023759192840972, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00014495125847024617}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 254ms/step - loss: 92.9025 - mae: 1.2681 - val_loss: 0.3946 - val_mae: 0.4651\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 92.2434 - mae: 1.3205 - val_loss: 0.6808 - val_mae: 0.7042\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 90.7997 - mae: 1.4521 - val_loss: 0.9713 - val_mae: 0.8694\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 87.9069 - mae: 1.5579 - val_loss: 1.1498 - val_mae: 0.9149\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 84.1770 - mae: 1.6191 - val_loss: 2.8085 - val_mae: 1.3301\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 80.4013 - mae: 1.6981 - val_loss: 2.7014 - val_mae: 1.1845\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 34s 262ms/step - loss: 77.8018 - mae: 1.6640 - val_loss: 5.3199 - val_mae: 1.5757\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 79.6593 - mae: 1.7311 - val_loss: 2.5509 - val_mae: 1.0858\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 76.3163 - mae: 1.5849 - val_loss: 4.3715 - val_mae: 1.3476\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 77.2072 - mae: 1.6434 - val_loss: 6.7490 - val_mae: 1.5580\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.3572 - mae: 1.6299 - val_loss: 6.3585 - val_mae: 1.4657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:23:07,796] Trial 42 finished with value: 0.3945711851119995 and parameters: {'l1': 1.91788112147795e-08, 'l2': 3.595416966564279e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.04594791100896272, 'dropout_rate': 0.12406533768497074, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.11610183619181287, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00012384187019666884}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 37s 254ms/step - loss: 93.0880 - mae: 1.2713 - val_loss: 0.4169 - val_mae: 0.4855\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 92.5645 - mae: 1.3431 - val_loss: 0.6585 - val_mae: 0.6923\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 91.7487 - mae: 1.4379 - val_loss: 0.7683 - val_mae: 0.7672\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 89.7686 - mae: 1.4929 - val_loss: 1.9708 - val_mae: 1.1652\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 86.1971 - mae: 1.6538 - val_loss: 4.1639 - val_mae: 1.3737\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 82.3918 - mae: 1.6564 - val_loss: 3.8625 - val_mae: 1.2153\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 78.9665 - mae: 1.6366 - val_loss: 8.3897 - val_mae: 1.4313\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 75.9063 - mae: 1.5920 - val_loss: 4.6300 - val_mae: 1.1654\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 79.7822 - mae: 1.5606 - val_loss: 2.2797 - val_mae: 0.9007\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 75.4324 - mae: 1.5827 - val_loss: 6.9005 - val_mae: 1.3474\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 76.8648 - mae: 1.5700 - val_loss: 6.7445 - val_mae: 1.4063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:29:09,487] Trial 43 finished with value: 0.4168512523174286 and parameters: {'l1': 1.535043587211427e-08, 'l2': 1.576621604713944e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.042161709230437015, 'dropout_rate': 0.12691941923459354, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.06210498593999115, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00012409608830630666}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 37s 254ms/step - loss: 93.0239 - mae: 1.2670 - val_loss: 0.3989 - val_mae: 0.4667\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 92.6460 - mae: 1.3028 - val_loss: 0.5388 - val_mae: 0.6061\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 92.1578 - mae: 1.3859 - val_loss: 0.6300 - val_mae: 0.6769\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 91.2257 - mae: 1.4248 - val_loss: 0.8753 - val_mae: 0.8140\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 89.8568 - mae: 1.4953 - val_loss: 1.0611 - val_mae: 0.8467\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 41s 322ms/step - loss: 87.7665 - mae: 1.5206 - val_loss: 2.1224 - val_mae: 1.0994\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 86.0279 - mae: 1.5606 - val_loss: 2.8660 - val_mae: 1.1314\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 83.8944 - mae: 1.5624 - val_loss: 3.5820 - val_mae: 1.1424\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 82.6552 - mae: 1.5859 - val_loss: 5.0261 - val_mae: 1.3419\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 34s 268ms/step - loss: 81.7299 - mae: 1.5574 - val_loss: 4.6146 - val_mae: 1.2246\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 79.2561 - mae: 1.5494 - val_loss: 5.3695 - val_mae: 1.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:35:24,334] Trial 44 finished with value: 0.3989143669605255 and parameters: {'l1': 5.6134160019361555e-08, 'l2': 4.420795252093383e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.07126124536158629, 'dropout_rate': 0.10018080081558356, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.4428703775095985, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00010231408661143761}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 262ms/step - loss: 92.3813 - mae: 1.2936 - val_loss: 0.5552 - val_mae: 0.6254\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 91.2088 - mae: 1.4082 - val_loss: 0.7033 - val_mae: 0.7278\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 89.9110 - mae: 1.4567 - val_loss: 1.0958 - val_mae: 0.9024\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 88.1751 - mae: 1.5224 - val_loss: 1.3087 - val_mae: 0.9305\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 86.4001 - mae: 1.5578 - val_loss: 1.9742 - val_mae: 1.0657\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 83.9621 - mae: 1.6033 - val_loss: 3.3401 - val_mae: 1.2733\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 82.8577 - mae: 1.6463 - val_loss: 3.7291 - val_mae: 1.2704\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 80.8813 - mae: 1.6444 - val_loss: 3.6774 - val_mae: 1.2072\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 79.5570 - mae: 1.5772 - val_loss: 5.1957 - val_mae: 1.3831\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 34s 266ms/step - loss: 78.6128 - mae: 1.6405 - val_loss: 4.6119 - val_mae: 1.2892\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 77.1226 - mae: 1.6282 - val_loss: 2.5044 - val_mae: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:41:38,243] Trial 45 finished with value: 0.5552293062210083 and parameters: {'l1': 1.0470287192160347e-08, 'l2': 2.5663393268104615e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.03102054673052261, 'dropout_rate': 0.12367217663671676, 'lstm_units_2': 16, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.11219478306124968, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'rmsprop', 'learning_rate': 0.00022997890681899556}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 259ms/step - loss: 78.1384 - mae: 1.2244 - val_loss: 0.3971 - val_mae: 0.4873\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 77.1633 - mae: 1.2263 - val_loss: 0.5548 - val_mae: 0.5925\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 82.1541 - mae: 1.2924 - val_loss: 0.7913 - val_mae: 0.6913\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 246ms/step - loss: 74.4471 - mae: 1.3519 - val_loss: 2.8267 - val_mae: 1.0882\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 86.2370 - mae: 1.4756 - val_loss: 1.3357 - val_mae: 0.8006\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 70.2597 - mae: 1.4154 - val_loss: 1.9599 - val_mae: 0.9059\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 72.5561 - mae: 1.4258 - val_loss: 2.0610 - val_mae: 0.9238\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 82.4391 - mae: 1.4849 - val_loss: 1.8661 - val_mae: 0.8627\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 75.5478 - mae: 1.4251 - val_loss: 2.5713 - val_mae: 0.9776\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 81.8187 - mae: 1.4536 - val_loss: 1.4121 - val_mae: 0.7859\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 78.4657 - mae: 1.4275 - val_loss: 3.4273 - val_mae: 1.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:47:41,856] Trial 46 finished with value: 0.3971185088157654 and parameters: {'l1': 3.8135212929187776e-08, 'l2': 3.682757218837557e-07, 'lstm_units_1': 128, 'activation_lstm': 'relu', 'recurrent_dropout_1': 0.10136567645263832, 'dropout_rate': 0.49697056585637955, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.12387634167402618, 'dense_units': 8, 'activation_dense': 'linear', 'optimizer': 'adam', 'learning_rate': 0.00029612334574043224}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 255ms/step - loss: 91.0482 - mae: 1.4225 - val_loss: 1.7305 - val_mae: 1.0337\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 82.0332 - mae: 1.5848 - val_loss: 2.8252 - val_mae: 1.2052\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 77.2684 - mae: 1.5676 - val_loss: 3.6217 - val_mae: 1.2604\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 76.5126 - mae: 1.5099 - val_loss: 2.9957 - val_mae: 1.0602\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 72.2319 - mae: 1.5600 - val_loss: 1.7260 - val_mae: 0.8976\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 71.4972 - mae: 1.5436 - val_loss: 3.1630 - val_mae: 0.8342\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 75.2217 - mae: 1.3550 - val_loss: 4.7420 - val_mae: 1.2756\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 73.2736 - mae: 1.5259 - val_loss: 2.2956 - val_mae: 1.0005\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 75.2907 - mae: 1.4806 - val_loss: 3.3109 - val_mae: 1.1461\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 71.2845 - mae: 1.4550 - val_loss: 1.8212 - val_mae: 0.8438\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 71.6186 - mae: 1.4287 - val_loss: 8.9841 - val_mae: 1.5261\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 71.6499 - mae: 1.3913 - val_loss: 1.9043 - val_mae: 0.8413\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 74.2942 - mae: 1.4081 - val_loss: 4.7173 - val_mae: 1.1780\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 71.3505 - mae: 1.4261 - val_loss: 2.6593 - val_mae: 0.9758\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 68.7689 - mae: 1.4204 - val_loss: 1.9624 - val_mae: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 20:55:55,329] Trial 47 finished with value: 1.726006031036377 and parameters: {'l1': 1.609310102164249e-08, 'l2': 1.2912106219843882e-07, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.13581295824268727, 'dropout_rate': 0.43717941243459124, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.03799359783705554, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0007763854572991278}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 38s 258ms/step - loss: 92.5445 - mae: 1.2955 - val_loss: 0.8828 - val_mae: 0.8164\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 86.4602 - mae: 1.5649 - val_loss: 2.7317 - val_mae: 0.8781\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 80.4272 - mae: 1.5922 - val_loss: 3.0745 - val_mae: 0.9182\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 77.1713 - mae: 1.5439 - val_loss: 2.9732 - val_mae: 1.0670\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 77.3206 - mae: 1.5398 - val_loss: 3.2960 - val_mae: 0.9576\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 75.3191 - mae: 1.5721 - val_loss: 9.7865 - val_mae: 2.1317\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 73.6526 - mae: 1.5450 - val_loss: 3.2611 - val_mae: 1.0591\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 73.2933 - mae: 1.5368 - val_loss: 1.9651 - val_mae: 0.8444\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 70.7537 - mae: 1.4585 - val_loss: 4.7214 - val_mae: 1.4268\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 69.9204 - mae: 1.5125 - val_loss: 4.7007 - val_mae: 0.8390\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 73.4837 - mae: 1.5066 - val_loss: 2.8765 - val_mae: 1.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 21:01:59,936] Trial 48 finished with value: 0.8828231692314148 and parameters: {'l1': 7.429795019302844e-08, 'l2': 6.328302656544882e-08, 'lstm_units_1': 64, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.06186202093598284, 'dropout_rate': 0.1173583684840945, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.0010631619149471527, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00034508596315262685}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "128/128 [==============================] - 39s 264ms/step - loss: 92.9441 - mae: 1.2717 - val_loss: 0.4442 - val_mae: 0.5234\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 92.5401 - mae: 1.3378 - val_loss: 0.6201 - val_mae: 0.6665\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 92.1556 - mae: 1.3987 - val_loss: 0.7382 - val_mae: 0.7487\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 91.6641 - mae: 1.4389 - val_loss: 1.0509 - val_mae: 0.8921\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 91.0872 - mae: 1.5044 - val_loss: 1.1618 - val_mae: 0.8827\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 90.4512 - mae: 1.5256 - val_loss: 1.2383 - val_mae: 0.8686\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 89.9582 - mae: 1.4929 - val_loss: 1.5588 - val_mae: 0.9449\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 89.5922 - mae: 1.5011 - val_loss: 2.1078 - val_mae: 1.0564\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 89.3230 - mae: 1.5432 - val_loss: 1.8190 - val_mae: 0.9416\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 89.0531 - mae: 1.5108 - val_loss: 2.1543 - val_mae: 1.0139\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 88.8859 - mae: 1.5385 - val_loss: 1.9780 - val_mae: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 21:08:04,051] Trial 49 finished with value: 0.4442257881164551 and parameters: {'l1': 2.0725827535071402e-08, 'l2': 1.9492876393439258e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.08225426905252003, 'dropout_rate': 0.15635727862533583, 'lstm_units_2': 16, 'activation_lstm_2': 'tanh', 'recurrent_dropout_2': 0.08139926240643466, 'dense_units': 16, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.00012708753581021367}. Best is trial 17 with value: 0.34048137068748474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros:\n",
      "{'l1': 3.3692472030315245e-07, 'l2': 8.7876011262569e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.26612587287797906, 'dropout_rate': 0.18703491200890998, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.10895054417461136, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0005130986400377099}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model(trial):\n",
    "\n",
    "    l1 = trial.suggest_float(\"l1\", 1e-8, 1e-3, log=True)\n",
    "    l2 = trial.suggest_float(\"l2\", 1e-8, 1e-2, log=True)\n",
    "    reg = regularizers.l1_l2(l1=l1, l2=l2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        units=trial.suggest_categorical(\"lstm_units_1\", [32, 64, 128]),\n",
    "        activation=trial.suggest_categorical(\"activation_lstm\", [\"tanh\", \"relu\"]),\n",
    "        input_shape=(14, 1),\n",
    "        return_sequences=True,\n",
    "        recurrent_dropout=trial.suggest_float(\"recurrent_dropout_1\", 0.0, 0.5),\n",
    "        kernel_regularizer=reg\n",
    "    ))\n",
    "    model.add(Dropout(trial.suggest_float(\"dropout_rate\", 0.1, 0.5)))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units=trial.suggest_categorical(\"lstm_units_2\", [16, 32, 64]),\n",
    "        activation=trial.suggest_categorical(\"activation_lstm_2\", [\"tanh\", \"relu\"]),\n",
    "        recurrent_dropout=trial.suggest_float(\"recurrent_dropout_2\", 0.0, 0.5),\n",
    "        kernel_regularizer=reg\n",
    "    ))\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=trial.suggest_categorical(\"dense_units\", [8, 16, 32]),\n",
    "        activation=trial.suggest_categorical(\"activation_dense\", [\"relu\", \"linear\"]),\n",
    "        kernel_regularizer=reg\n",
    "    ))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\"])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model = build_model(trial)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=200,\n",
    "        batch_size=128,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    val_loss = min(history.history[\"val_loss\"])\n",
    "    return val_loss\n",
    "\n",
    "os.makedirs(\"optuna_storage\", exist_ok=True)\n",
    "DB_PATH = \"optuna_storage/bbdd.db\"\n",
    "STUDY_NAME = study_name\n",
    "storage_url = f\"sqlite:///{DB_PATH}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage_url,\n",
    "    direction=\"minimize\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "\n",
    "# Ejecutar la búsqueda\n",
    "\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Mostrar mejores parámetros\n",
    "print(\"Mejores parámetros:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb066de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados:\n",
      "{'l1': 3.3692472030315245e-07, 'l2': 8.7876011262569e-08, 'lstm_units_1': 32, 'activation_lstm': 'tanh', 'recurrent_dropout_1': 0.26612587287797906, 'dropout_rate': 0.18703491200890998, 'lstm_units_2': 32, 'activation_lstm_2': 'relu', 'recurrent_dropout_2': 0.10895054417461136, 'dense_units': 8, 'activation_dense': 'relu', 'optimizer': 'adam', 'learning_rate': 0.0005130986400377099}\n"
     ]
    }
   ],
   "source": [
    "# Ruta a la base de datos\n",
    "DB_PATH = \"optuna_storage/bbdd.db\"\n",
    "\n",
    "# Cargar el estudio almacenado\n",
    "study = optuna.load_study(\n",
    "    study_name=study_name,  # Usar None si solo hay un estudio en la DB, o especificar el nombre\n",
    "    storage=f\"sqlite:///{DB_PATH}\"\n",
    ")\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a5de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 14, 32)            4352      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 32)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,945\n",
      "Trainable params: 12,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l1 = best_params[\"l1\"]\n",
    "l2 = best_params[\"l2\"]\n",
    "reg = regularizers.l1_l2(l1=l1, l2=l2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units=best_params[\"lstm_units_1\"],\n",
    "    activation=best_params[\"activation_lstm\"],\n",
    "    input_shape=(14, 1),\n",
    "    return_sequences=True,\n",
    "    recurrent_dropout=best_params[\"recurrent_dropout_1\"],\n",
    "    kernel_regularizer=reg\n",
    "))\n",
    "model.add(Dropout(best_params[\"dropout_rate\"]))\n",
    "\n",
    "model.add(LSTM(\n",
    "    units=best_params[\"lstm_units_2\"],\n",
    "    activation=best_params[\"activation_lstm_2\"],\n",
    "    recurrent_dropout=best_params[\"recurrent_dropout_2\"],\n",
    "    kernel_regularizer=reg\n",
    "))\n",
    "\n",
    "model.add(Dense(\n",
    "    units=best_params[\"dense_units\"],\n",
    "    activation=best_params[\"activation_dense\"],\n",
    "    kernel_regularizer=reg\n",
    "))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "optimizer_name = best_params[\"optimizer\"]\n",
    "learning_rate = best_params[\"learning_rate\"]\n",
    "\n",
    "if optimizer_name == \"adam\":\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "elif optimizer_name == \"rmsprop\":\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc99ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "128/128 [==============================] - 41s 250ms/step - loss: 92.4079 - mae: 1.3757 - val_loss: 1.0713 - val_mae: 0.9333\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 87.6363 - mae: 1.6160 - val_loss: 1.4445 - val_mae: 0.9722\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 30s 238ms/step - loss: 82.2088 - mae: 1.6940 - val_loss: 1.4131 - val_mae: 0.9376\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 31s 238ms/step - loss: 79.3094 - mae: 1.6312 - val_loss: 6.1503 - val_mae: 1.6223\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 30s 237ms/step - loss: 75.5787 - mae: 1.6375 - val_loss: 2.1370 - val_mae: 0.9408\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 30s 237ms/step - loss: 74.2910 - mae: 1.5644 - val_loss: 5.7674 - val_mae: 1.4673\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 30s 236ms/step - loss: 80.9130 - mae: 1.3536 - val_loss: 0.4942 - val_mae: 0.4815\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 75.5911 - mae: 1.3829 - val_loss: 1.9854 - val_mae: 0.9080\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 30s 237ms/step - loss: 70.4710 - mae: 1.5205 - val_loss: 0.8558 - val_mae: 0.6516\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 74.8417 - mae: 1.4206 - val_loss: 3.7136 - val_mae: 1.1765\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 35s 271ms/step - loss: 71.3486 - mae: 1.4686 - val_loss: 1.0840 - val_mae: 0.6941\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 34s 266ms/step - loss: 73.2692 - mae: 1.4463 - val_loss: 4.9880 - val_mae: 1.3098\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 34s 266ms/step - loss: 71.8207 - mae: 1.4096 - val_loss: 2.7744 - val_mae: 1.0660\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 34s 266ms/step - loss: 72.8242 - mae: 1.3923 - val_loss: 2.7989 - val_mae: 1.0730\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 34s 268ms/step - loss: 74.4306 - mae: 1.4257 - val_loss: 0.9331 - val_mae: 0.6874\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 71.6189 - mae: 1.3976 - val_loss: 5.2434 - val_mae: 1.2971\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 34s 268ms/step - loss: 72.1270 - mae: 1.3957 - val_loss: 6.1246 - val_mae: 1.4484\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 34s 267ms/step - loss: 70.1143 - mae: 1.4106 - val_loss: 6.7140 - val_mae: 1.4906\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 68.9482 - mae: 1.4619 - val_loss: 6.1614 - val_mae: 1.4487\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 69.0683 - mae: 1.4044 - val_loss: 3.9148 - val_mae: 1.2182\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 35s 271ms/step - loss: 70.8367 - mae: 1.3890 - val_loss: 2.1071 - val_mae: 0.9169\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 35s 275ms/step - loss: 68.0030 - mae: 1.4196 - val_loss: 1.8463 - val_mae: 0.9195\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 35s 272ms/step - loss: 68.1428 - mae: 1.4532 - val_loss: 3.4994 - val_mae: 1.1207\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 68.4554 - mae: 1.4339 - val_loss: 1.6145 - val_mae: 0.7978\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 34s 265ms/step - loss: 68.6698 - mae: 1.3388 - val_loss: 3.4181 - val_mae: 1.0875\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 72.6508 - mae: 1.3171 - val_loss: 2.3923 - val_mae: 0.9267\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 64.3073 - mae: 1.4039 - val_loss: 4.1220 - val_mae: 1.2797\n",
      "Epoch 28/200\n",
      "128/128 [==============================] - 29s 230ms/step - loss: 69.1926 - mae: 1.4603 - val_loss: 2.6824 - val_mae: 0.9880\n",
      "Epoch 29/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 66.0878 - mae: 1.3991 - val_loss: 3.0254 - val_mae: 1.0513\n",
      "Epoch 30/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 69.3597 - mae: 1.3058 - val_loss: 3.1577 - val_mae: 1.0665\n",
      "Epoch 31/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 67.8637 - mae: 1.4412 - val_loss: 6.0417 - val_mae: 1.3630\n",
      "Epoch 32/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 66.8989 - mae: 1.4310 - val_loss: 3.7742 - val_mae: 1.2087\n",
      "Epoch 33/200\n",
      "128/128 [==============================] - 34s 262ms/step - loss: 69.8215 - mae: 1.4572 - val_loss: 2.7137 - val_mae: 1.0198\n",
      "Epoch 34/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 66.4404 - mae: 1.3816 - val_loss: 3.0145 - val_mae: 1.0575\n",
      "Epoch 35/200\n",
      "128/128 [==============================] - 35s 270ms/step - loss: 66.2664 - mae: 1.3768 - val_loss: 1.9095 - val_mae: 0.8791\n",
      "Epoch 36/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 64.9090 - mae: 1.4146 - val_loss: 1.5065 - val_mae: 0.8350\n",
      "Epoch 37/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 68.2275 - mae: 1.3632 - val_loss: 3.2964 - val_mae: 1.0395\n",
      "Epoch 38/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 71.0058 - mae: 1.3957 - val_loss: 2.7427 - val_mae: 1.0102\n",
      "Epoch 39/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 70.3814 - mae: 1.3913 - val_loss: 2.4848 - val_mae: 0.9715\n",
      "Epoch 40/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 67.9539 - mae: 1.4386 - val_loss: 2.1455 - val_mae: 0.9424\n",
      "Epoch 41/200\n",
      "128/128 [==============================] - 34s 264ms/step - loss: 70.5079 - mae: 1.3937 - val_loss: 1.7504 - val_mae: 0.7928\n",
      "Epoch 42/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 67.7712 - mae: 1.3594 - val_loss: 3.9734 - val_mae: 1.1724\n",
      "Epoch 43/200\n",
      "128/128 [==============================] - 33s 256ms/step - loss: 63.9467 - mae: 1.3712 - val_loss: 2.7113 - val_mae: 0.9789\n",
      "Epoch 44/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 64.8017 - mae: 1.4089 - val_loss: 5.7267 - val_mae: 1.4020\n",
      "Epoch 45/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 69.4242 - mae: 1.4174 - val_loss: 2.3882 - val_mae: 0.8981\n",
      "Epoch 46/200\n",
      "128/128 [==============================] - 31s 240ms/step - loss: 73.9065 - mae: 1.3402 - val_loss: 3.4559 - val_mae: 1.0670\n",
      "Epoch 47/200\n",
      "128/128 [==============================] - 34s 267ms/step - loss: 68.3275 - mae: 1.3471 - val_loss: 1.0493 - val_mae: 0.7268\n",
      "Epoch 48/200\n",
      "128/128 [==============================] - 31s 238ms/step - loss: 70.1531 - mae: 1.4192 - val_loss: 1.4385 - val_mae: 0.7957\n",
      "Epoch 49/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 67.0324 - mae: 1.3931 - val_loss: 3.0268 - val_mae: 1.0520\n",
      "Epoch 50/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 63.9330 - mae: 1.3893 - val_loss: 1.8481 - val_mae: 0.8709\n",
      "Epoch 51/200\n",
      "128/128 [==============================] - 31s 240ms/step - loss: 67.4403 - mae: 1.2897 - val_loss: 2.1948 - val_mae: 0.9496\n",
      "Epoch 52/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 63.9433 - mae: 1.4453 - val_loss: 2.0548 - val_mae: 0.9150\n",
      "Epoch 53/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 67.1813 - mae: 1.3759 - val_loss: 1.9039 - val_mae: 0.8917\n",
      "Epoch 54/200\n",
      "128/128 [==============================] - 31s 240ms/step - loss: 67.6605 - mae: 1.3729 - val_loss: 2.0237 - val_mae: 0.9256\n",
      "Epoch 55/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 69.8144 - mae: 1.3730 - val_loss: 4.1773 - val_mae: 1.0898\n",
      "Epoch 56/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 63.8052 - mae: 1.3508 - val_loss: 2.2159 - val_mae: 0.9475\n",
      "Epoch 57/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 68.7351 - mae: 1.4465 - val_loss: 1.0953 - val_mae: 0.7205\n",
      "Epoch 58/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 68.2511 - mae: 1.2834 - val_loss: 2.4644 - val_mae: 0.9572\n",
      "Epoch 59/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 64.9507 - mae: 1.3454 - val_loss: 1.1639 - val_mae: 0.7533\n",
      "Epoch 60/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 68.5370 - mae: 1.3415 - val_loss: 2.1227 - val_mae: 0.9175\n",
      "Epoch 61/200\n",
      "128/128 [==============================] - 31s 243ms/step - loss: 66.9160 - mae: 1.3671 - val_loss: 2.9958 - val_mae: 1.0182\n",
      "Epoch 62/200\n",
      "128/128 [==============================] - 31s 244ms/step - loss: 64.4974 - mae: 1.3411 - val_loss: 1.7396 - val_mae: 0.8361\n",
      "Epoch 63/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 65.0167 - mae: 1.2897 - val_loss: 2.2164 - val_mae: 0.8878\n",
      "Epoch 64/200\n",
      "128/128 [==============================] - 33s 254ms/step - loss: 69.7161 - mae: 1.4358 - val_loss: 2.4236 - val_mae: 0.9947\n",
      "Epoch 65/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 67.9688 - mae: 1.3921 - val_loss: 2.5080 - val_mae: 0.9644\n",
      "Epoch 66/200\n",
      "128/128 [==============================] - 34s 264ms/step - loss: 70.3279 - mae: 1.3699 - val_loss: 2.3708 - val_mae: 0.9417\n",
      "Epoch 67/200\n",
      "128/128 [==============================] - 31s 244ms/step - loss: 64.5294 - mae: 1.3542 - val_loss: 1.8308 - val_mae: 0.8536\n",
      "Epoch 68/200\n",
      "128/128 [==============================] - 33s 261ms/step - loss: 65.7156 - mae: 1.3412 - val_loss: 2.5214 - val_mae: 0.9677\n",
      "Epoch 69/200\n",
      "128/128 [==============================] - 34s 264ms/step - loss: 64.9583 - mae: 1.3666 - val_loss: 1.2101 - val_mae: 0.7562\n",
      "Epoch 70/200\n",
      "128/128 [==============================] - 31s 242ms/step - loss: 63.9200 - mae: 1.3678 - val_loss: 2.6244 - val_mae: 0.9945\n",
      "Epoch 71/200\n",
      "128/128 [==============================] - 30s 238ms/step - loss: 65.9455 - mae: 1.3040 - val_loss: 1.7165 - val_mae: 0.8078\n",
      "Epoch 72/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 63.4745 - mae: 1.3148 - val_loss: 1.8988 - val_mae: 0.8519\n",
      "Epoch 73/200\n",
      "128/128 [==============================] - 31s 241ms/step - loss: 67.7339 - mae: 1.3030 - val_loss: 2.1707 - val_mae: 0.8918\n",
      "Epoch 74/200\n",
      "128/128 [==============================] - 34s 262ms/step - loss: 65.8775 - mae: 1.3620 - val_loss: 1.5863 - val_mae: 0.8064\n",
      "Epoch 75/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 64.5132 - mae: 1.3107 - val_loss: 2.3071 - val_mae: 0.8959\n",
      "Epoch 76/200\n",
      "128/128 [==============================] - 31s 239ms/step - loss: 64.7920 - mae: 1.3651 - val_loss: 2.3272 - val_mae: 0.9584\n",
      "Epoch 77/200\n",
      "128/128 [==============================] - 30s 236ms/step - loss: 65.1884 - mae: 1.4256 - val_loss: 3.4347 - val_mae: 1.1273\n",
      "Epoch 78/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 71.9376 - mae: 1.3650 - val_loss: 1.4806 - val_mae: 0.7962\n",
      "Epoch 79/200\n",
      "128/128 [==============================] - 31s 244ms/step - loss: 62.0860 - mae: 1.3310 - val_loss: 2.1325 - val_mae: 0.8894\n",
      "Epoch 80/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 64.7597 - mae: 1.3048 - val_loss: 1.6927 - val_mae: 0.8169\n",
      "Epoch 81/200\n",
      "128/128 [==============================] - 31s 243ms/step - loss: 64.2639 - mae: 1.3362 - val_loss: 2.4020 - val_mae: 0.9331\n",
      "Epoch 82/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 66.7982 - mae: 1.3708 - val_loss: 2.1960 - val_mae: 0.8868\n",
      "Epoch 83/200\n",
      "128/128 [==============================] - 34s 266ms/step - loss: 63.5704 - mae: 1.3675 - val_loss: 2.0877 - val_mae: 0.9031\n",
      "Epoch 84/200\n",
      "128/128 [==============================] - 34s 265ms/step - loss: 64.0180 - mae: 1.3965 - val_loss: 2.0458 - val_mae: 0.9000\n",
      "Epoch 85/200\n",
      "128/128 [==============================] - 36s 280ms/step - loss: 62.3773 - mae: 1.3876 - val_loss: 3.9415 - val_mae: 1.1004\n",
      "Epoch 86/200\n",
      "128/128 [==============================] - 35s 271ms/step - loss: 62.1646 - mae: 1.3894 - val_loss: 1.9719 - val_mae: 0.8888\n",
      "Epoch 87/200\n",
      "128/128 [==============================] - 35s 272ms/step - loss: 66.9279 - mae: 1.3407 - val_loss: 1.8140 - val_mae: 0.8494\n",
      "Epoch 88/200\n",
      "128/128 [==============================] - 35s 277ms/step - loss: 62.7237 - mae: 1.3533 - val_loss: 2.4992 - val_mae: 0.9415\n",
      "Epoch 89/200\n",
      "128/128 [==============================] - 34s 265ms/step - loss: 65.1875 - mae: 1.3942 - val_loss: 2.0049 - val_mae: 0.8948\n",
      "Epoch 90/200\n",
      "128/128 [==============================] - 33s 260ms/step - loss: 66.0864 - mae: 1.3733 - val_loss: 2.2741 - val_mae: 0.9225\n",
      "Epoch 91/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 61.9892 - mae: 1.3717 - val_loss: 2.7268 - val_mae: 0.9715\n",
      "Epoch 92/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 63.4331 - mae: 1.3841 - val_loss: 3.2712 - val_mae: 1.0093\n",
      "Epoch 93/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 64.3744 - mae: 1.4408 - val_loss: 2.4433 - val_mae: 0.9158\n",
      "Epoch 94/200\n",
      "128/128 [==============================] - 32s 249ms/step - loss: 63.6466 - mae: 1.3572 - val_loss: 3.2647 - val_mae: 0.9739\n",
      "Epoch 95/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 61.8691 - mae: 1.3942 - val_loss: 1.9304 - val_mae: 0.8690\n",
      "Epoch 96/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 59.3838 - mae: 1.3587 - val_loss: 3.0296 - val_mae: 1.0183\n",
      "Epoch 97/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 62.3114 - mae: 1.3627 - val_loss: 2.1664 - val_mae: 0.9015\n",
      "Epoch 98/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 70.8639 - mae: 1.3955 - val_loss: 2.3255 - val_mae: 0.9030\n",
      "Epoch 99/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 66.4363 - mae: 1.3484 - val_loss: 2.2756 - val_mae: 0.9157\n",
      "Epoch 100/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 64.3412 - mae: 1.3793 - val_loss: 2.5356 - val_mae: 0.9875\n",
      "Epoch 101/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 63.3062 - mae: 1.3914 - val_loss: 3.0605 - val_mae: 1.0387\n",
      "Epoch 102/200\n",
      "128/128 [==============================] - 31s 244ms/step - loss: 62.4190 - mae: 1.4278 - val_loss: 2.0877 - val_mae: 0.9220\n",
      "Epoch 103/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 60.1576 - mae: 1.3853 - val_loss: 2.6032 - val_mae: 0.9996\n",
      "Epoch 104/200\n",
      "128/128 [==============================] - 32s 254ms/step - loss: 63.2604 - mae: 1.4216 - val_loss: 2.0146 - val_mae: 0.8997\n",
      "Epoch 105/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 62.0402 - mae: 1.3812 - val_loss: 1.8312 - val_mae: 0.8709\n",
      "Epoch 106/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 63.9157 - mae: 1.4018 - val_loss: 1.8126 - val_mae: 0.8658\n",
      "Epoch 107/200\n",
      "128/128 [==============================] - 34s 264ms/step - loss: 61.7232 - mae: 1.3448 - val_loss: 1.5396 - val_mae: 0.8131\n",
      "Epoch 108/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 61.7813 - mae: 1.3546 - val_loss: 1.9887 - val_mae: 0.8885\n",
      "Epoch 109/200\n",
      "128/128 [==============================] - 34s 262ms/step - loss: 64.8565 - mae: 1.3719 - val_loss: 1.7340 - val_mae: 0.8335\n",
      "Epoch 110/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 62.2901 - mae: 1.3545 - val_loss: 1.5934 - val_mae: 0.8199\n",
      "Epoch 111/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 62.6537 - mae: 1.3917 - val_loss: 2.3752 - val_mae: 0.9194\n",
      "Epoch 112/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 62.8700 - mae: 1.3865 - val_loss: 2.4084 - val_mae: 0.9440\n",
      "Epoch 113/200\n",
      "128/128 [==============================] - 34s 265ms/step - loss: 68.8487 - mae: 1.3939 - val_loss: 2.2657 - val_mae: 0.9107\n",
      "Epoch 114/200\n",
      "128/128 [==============================] - 34s 262ms/step - loss: 61.4383 - mae: 1.3751 - val_loss: 2.3294 - val_mae: 0.9158\n",
      "Epoch 115/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 61.7999 - mae: 1.3653 - val_loss: 2.5107 - val_mae: 0.9229\n",
      "Epoch 116/200\n",
      "128/128 [==============================] - 31s 241ms/step - loss: 66.0989 - mae: 1.4334 - val_loss: 1.8362 - val_mae: 0.8649\n",
      "Epoch 117/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 61.0921 - mae: 1.3882 - val_loss: 2.0307 - val_mae: 0.9025\n",
      "Epoch 118/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 68.4537 - mae: 1.4093 - val_loss: 1.9536 - val_mae: 0.8823\n",
      "Epoch 119/200\n",
      "128/128 [==============================] - 31s 243ms/step - loss: 62.0712 - mae: 1.3831 - val_loss: 2.0282 - val_mae: 0.8825\n",
      "Epoch 120/200\n",
      "128/128 [==============================] - 30s 235ms/step - loss: 59.8033 - mae: 1.3800 - val_loss: 2.1950 - val_mae: 0.9219\n",
      "Epoch 121/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 64.7083 - mae: 1.4182 - val_loss: 1.8442 - val_mae: 0.8782\n",
      "Epoch 122/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 61.9503 - mae: 1.3601 - val_loss: 1.7738 - val_mae: 0.8390\n",
      "Epoch 123/200\n",
      "128/128 [==============================] - 32s 250ms/step - loss: 61.6771 - mae: 1.3556 - val_loss: 2.5263 - val_mae: 0.9310\n",
      "Epoch 124/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 61.1556 - mae: 1.3964 - val_loss: 1.9978 - val_mae: 0.8954\n",
      "Epoch 125/200\n",
      "128/128 [==============================] - 32s 252ms/step - loss: 61.4170 - mae: 1.3917 - val_loss: 2.3283 - val_mae: 0.9297\n",
      "Epoch 126/200\n",
      "128/128 [==============================] - 31s 244ms/step - loss: 69.4372 - mae: 1.3537 - val_loss: 1.9895 - val_mae: 0.8718\n",
      "Epoch 127/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 63.8975 - mae: 1.3617 - val_loss: 2.0801 - val_mae: 0.8895\n",
      "Epoch 128/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 62.1068 - mae: 1.3829 - val_loss: 2.4087 - val_mae: 0.9376\n",
      "Epoch 129/200\n",
      "128/128 [==============================] - 34s 269ms/step - loss: 60.9029 - mae: 1.3974 - val_loss: 1.8944 - val_mae: 0.8785\n",
      "Epoch 130/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 64.4258 - mae: 1.4027 - val_loss: 2.3569 - val_mae: 0.9312\n",
      "Epoch 131/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 63.2793 - mae: 1.4045 - val_loss: 2.0727 - val_mae: 0.9127\n",
      "Epoch 132/200\n",
      "128/128 [==============================] - 30s 234ms/step - loss: 62.1067 - mae: 1.4040 - val_loss: 2.2158 - val_mae: 0.9304\n",
      "Epoch 133/200\n",
      "128/128 [==============================] - 30s 234ms/step - loss: 60.6979 - mae: 1.4035 - val_loss: 1.8558 - val_mae: 0.8778\n",
      "Epoch 134/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 64.7690 - mae: 1.4047 - val_loss: 1.8916 - val_mae: 0.8886\n",
      "Epoch 135/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 63.2878 - mae: 1.4163 - val_loss: 2.0424 - val_mae: 0.9162\n",
      "Epoch 136/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 64.7949 - mae: 1.3842 - val_loss: 1.9456 - val_mae: 0.8784\n",
      "Epoch 137/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 61.8650 - mae: 1.3823 - val_loss: 2.3553 - val_mae: 0.9512\n",
      "Epoch 138/200\n",
      "128/128 [==============================] - 31s 241ms/step - loss: 60.4102 - mae: 1.3890 - val_loss: 1.6122 - val_mae: 0.8108\n",
      "Epoch 139/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 59.5058 - mae: 1.3279 - val_loss: 2.0922 - val_mae: 0.9003\n",
      "Epoch 140/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 58.6224 - mae: 1.3631 - val_loss: 1.9553 - val_mae: 0.8943\n",
      "Epoch 141/200\n",
      "128/128 [==============================] - 33s 255ms/step - loss: 60.2317 - mae: 1.3824 - val_loss: 1.9728 - val_mae: 0.8957\n",
      "Epoch 142/200\n",
      "128/128 [==============================] - 34s 265ms/step - loss: 62.6271 - mae: 1.4082 - val_loss: 1.5886 - val_mae: 0.7920\n",
      "Epoch 143/200\n",
      "128/128 [==============================] - 34s 264ms/step - loss: 60.8861 - mae: 1.3187 - val_loss: 1.8843 - val_mae: 0.8698\n",
      "Epoch 144/200\n",
      "128/128 [==============================] - 33s 257ms/step - loss: 61.4015 - mae: 1.3700 - val_loss: 2.1621 - val_mae: 0.9078\n",
      "Epoch 145/200\n",
      "128/128 [==============================] - 31s 244ms/step - loss: 61.4146 - mae: 1.3734 - val_loss: 1.8598 - val_mae: 0.8754\n",
      "Epoch 146/200\n",
      "128/128 [==============================] - 35s 270ms/step - loss: 66.6307 - mae: 1.3804 - val_loss: 1.7427 - val_mae: 0.8287\n",
      "Epoch 147/200\n",
      "128/128 [==============================] - 35s 275ms/step - loss: 65.2006 - mae: 1.3702 - val_loss: 1.7224 - val_mae: 0.8224\n",
      "Epoch 148/200\n",
      "128/128 [==============================] - 34s 263ms/step - loss: 60.9857 - mae: 1.3386 - val_loss: 1.8666 - val_mae: 0.8722\n",
      "Epoch 149/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 62.3260 - mae: 1.3793 - val_loss: 2.0530 - val_mae: 0.8897\n",
      "Epoch 150/200\n",
      "128/128 [==============================] - 34s 262ms/step - loss: 60.3392 - mae: 1.3890 - val_loss: 2.1442 - val_mae: 0.9263\n",
      "Epoch 151/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 58.8133 - mae: 1.3939 - val_loss: 2.0661 - val_mae: 0.9097\n",
      "Epoch 152/200\n",
      "128/128 [==============================] - 31s 242ms/step - loss: 58.5971 - mae: 1.3771 - val_loss: 2.0721 - val_mae: 0.9105\n",
      "Epoch 153/200\n",
      "128/128 [==============================] - 31s 244ms/step - loss: 60.1713 - mae: 1.3665 - val_loss: 1.6090 - val_mae: 0.8216\n",
      "Epoch 154/200\n",
      "128/128 [==============================] - 31s 240ms/step - loss: 61.2956 - mae: 1.3572 - val_loss: 1.9409 - val_mae: 0.8585\n",
      "Epoch 155/200\n",
      "128/128 [==============================] - 31s 243ms/step - loss: 61.1764 - mae: 1.3581 - val_loss: 1.9681 - val_mae: 0.8771\n",
      "Epoch 156/200\n",
      "128/128 [==============================] - 32s 251ms/step - loss: 59.0541 - mae: 1.3457 - val_loss: 2.1310 - val_mae: 0.9088\n",
      "Epoch 157/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 59.3297 - mae: 1.3631 - val_loss: 1.9069 - val_mae: 0.8813\n",
      "Epoch 158/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 58.1333 - mae: 1.3638 - val_loss: 2.2196 - val_mae: 0.9204\n",
      "Epoch 159/200\n",
      "128/128 [==============================] - 31s 241ms/step - loss: 60.4434 - mae: 1.3483 - val_loss: 2.0385 - val_mae: 0.9011\n",
      "Epoch 160/200\n",
      "128/128 [==============================] - 31s 243ms/step - loss: 59.2600 - mae: 1.3627 - val_loss: 1.9580 - val_mae: 0.8923\n",
      "Epoch 161/200\n",
      "128/128 [==============================] - 30s 237ms/step - loss: 58.7760 - mae: 1.3408 - val_loss: 1.8255 - val_mae: 0.8619\n",
      "Epoch 162/200\n",
      "128/128 [==============================] - 31s 239ms/step - loss: 63.8282 - mae: 1.3782 - val_loss: 1.9738 - val_mae: 0.8911\n",
      "Epoch 163/200\n",
      "128/128 [==============================] - 32s 253ms/step - loss: 64.7613 - mae: 1.3878 - val_loss: 2.2759 - val_mae: 0.9173\n",
      "Epoch 164/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 60.6526 - mae: 1.3844 - val_loss: 1.9605 - val_mae: 0.8904\n",
      "Epoch 165/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 61.0198 - mae: 1.3933 - val_loss: 2.1144 - val_mae: 0.9111\n",
      "Epoch 166/200\n",
      "128/128 [==============================] - 31s 246ms/step - loss: 62.5877 - mae: 1.4030 - val_loss: 2.1321 - val_mae: 0.9112\n",
      "Epoch 167/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 63.5622 - mae: 1.3946 - val_loss: 1.8450 - val_mae: 0.8594\n",
      "Epoch 168/200\n",
      "128/128 [==============================] - 32s 247ms/step - loss: 60.1911 - mae: 1.3810 - val_loss: 2.2809 - val_mae: 0.9183\n",
      "Epoch 169/200\n",
      "128/128 [==============================] - 31s 239ms/step - loss: 59.0494 - mae: 1.3786 - val_loss: 1.8804 - val_mae: 0.8666\n",
      "Epoch 170/200\n",
      "128/128 [==============================] - 33s 258ms/step - loss: 57.6530 - mae: 1.3398 - val_loss: 2.3841 - val_mae: 0.9426\n",
      "Epoch 171/200\n",
      "128/128 [==============================] - 31s 242ms/step - loss: 57.1969 - mae: 1.3419 - val_loss: 2.3462 - val_mae: 0.9433\n",
      "Epoch 172/200\n",
      "128/128 [==============================] - 32s 248ms/step - loss: 66.4731 - mae: 1.3921 - val_loss: 2.0355 - val_mae: 0.8856\n",
      "Epoch 173/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 60.0314 - mae: 1.3484 - val_loss: 2.1520 - val_mae: 0.9035\n",
      "Epoch 174/200\n",
      "128/128 [==============================] - 31s 243ms/step - loss: 58.3567 - mae: 1.3498 - val_loss: 1.9249 - val_mae: 0.8686\n",
      "Epoch 175/200\n",
      "128/128 [==============================] - 31s 239ms/step - loss: 59.9169 - mae: 1.3378 - val_loss: 1.9292 - val_mae: 0.8658\n",
      "Epoch 176/200\n",
      "128/128 [==============================] - 33s 259ms/step - loss: 59.9201 - mae: 1.3174 - val_loss: 1.7960 - val_mae: 0.8448\n",
      "Epoch 177/200\n",
      "128/128 [==============================] - 31s 240ms/step - loss: 58.0241 - mae: 1.3414 - val_loss: 2.0683 - val_mae: 0.9076\n",
      "Epoch 178/200\n",
      "128/128 [==============================] - 30s 238ms/step - loss: 58.1518 - mae: 1.3660 - val_loss: 1.9755 - val_mae: 0.8854\n",
      "Epoch 179/200\n",
      "128/128 [==============================] - 29s 228ms/step - loss: 62.1273 - mae: 1.3736 - val_loss: 2.2440 - val_mae: 0.9261\n",
      "Epoch 180/200\n",
      "128/128 [==============================] - 31s 241ms/step - loss: 57.1383 - mae: 1.3212 - val_loss: 2.0505 - val_mae: 0.8988\n",
      "Epoch 181/200\n",
      "128/128 [==============================] - 31s 241ms/step - loss: 58.1745 - mae: 1.3393 - val_loss: 2.0787 - val_mae: 0.9031\n",
      "Epoch 182/200\n",
      "128/128 [==============================] - 30s 233ms/step - loss: 59.9061 - mae: 1.3694 - val_loss: 2.1949 - val_mae: 0.9340\n",
      "Epoch 183/200\n",
      "128/128 [==============================] - 31s 242ms/step - loss: 60.1843 - mae: 1.3649 - val_loss: 1.8815 - val_mae: 0.8905\n",
      "Epoch 184/200\n",
      "128/128 [==============================] - 31s 245ms/step - loss: 60.5210 - mae: 1.3338 - val_loss: 2.2467 - val_mae: 0.9238\n",
      "Epoch 185/200\n",
      "128/128 [==============================] - 30s 233ms/step - loss: 57.6593 - mae: 1.3519 - val_loss: 2.2474 - val_mae: 0.9406\n",
      "Epoch 186/200\n",
      "128/128 [==============================] - 30s 237ms/step - loss: 63.6418 - mae: 1.3760 - val_loss: 2.3296 - val_mae: 0.9098\n",
      "Epoch 187/200\n",
      "128/128 [==============================] - 29s 230ms/step - loss: 72.8170 - mae: 1.3606 - val_loss: 1.9287 - val_mae: 0.8777\n",
      "Epoch 188/200\n",
      "128/128 [==============================] - 30s 231ms/step - loss: 61.0014 - mae: 1.3563 - val_loss: 2.3006 - val_mae: 0.9237\n",
      "Epoch 189/200\n",
      "128/128 [==============================] - 29s 230ms/step - loss: 58.4685 - mae: 1.3558 - val_loss: 2.2127 - val_mae: 0.9187\n",
      "Epoch 190/200\n",
      "128/128 [==============================] - 30s 234ms/step - loss: 60.1572 - mae: 1.3691 - val_loss: 1.8475 - val_mae: 0.8678\n",
      "Epoch 191/200\n",
      "128/128 [==============================] - 30s 236ms/step - loss: 58.5297 - mae: 1.3403 - val_loss: 2.0985 - val_mae: 0.9171\n",
      "Epoch 192/200\n",
      "128/128 [==============================] - 30s 235ms/step - loss: 57.1904 - mae: 1.3544 - val_loss: 2.0336 - val_mae: 0.8930\n",
      "Epoch 193/200\n",
      "128/128 [==============================] - 30s 232ms/step - loss: 57.5022 - mae: 1.3618 - val_loss: 1.9879 - val_mae: 0.8893\n",
      "Epoch 194/200\n",
      "128/128 [==============================] - 30s 235ms/step - loss: 58.3765 - mae: 1.3546 - val_loss: 1.9951 - val_mae: 0.8971\n",
      "Epoch 195/200\n",
      "128/128 [==============================] - 30s 236ms/step - loss: 59.2721 - mae: 1.3876 - val_loss: 2.4389 - val_mae: 1.0397\n",
      "Epoch 196/200\n",
      "128/128 [==============================] - 30s 233ms/step - loss: 68.4540 - mae: 1.4340 - val_loss: 2.0483 - val_mae: 0.8793\n",
      "Epoch 197/200\n",
      "128/128 [==============================] - 31s 238ms/step - loss: 61.3665 - mae: 1.3737 - val_loss: 1.7546 - val_mae: 0.8236\n",
      "Epoch 198/200\n",
      "128/128 [==============================] - 30s 235ms/step - loss: 61.2920 - mae: 1.3406 - val_loss: 1.8319 - val_mae: 0.8458\n",
      "Epoch 199/200\n",
      "128/128 [==============================] - 30s 234ms/step - loss: 58.5170 - mae: 1.3310 - val_loss: 1.9943 - val_mae: 0.8868\n",
      "Epoch 200/200\n",
      "128/128 [==============================] - 30s 237ms/step - loss: 58.2401 - mae: 1.3424 - val_loss: 2.0497 - val_mae: 0.8893\n",
      "Mejor validación MSE: 0.4942\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    "    #callbacks=[early_stopping]\n",
    ")\n",
    "val_loss = min(history.history[\"val_loss\"])\n",
    "print(f\"Mejor validación MSE: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4312b2ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predecir_t2() missing 2 required positional arguments: 'mes' and 'quarter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m producto_ejemplo \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20001\u001b[39m\n\u001b[0;32m     20\u001b[0m ultimos_12_ejemplo \u001b[38;5;241m=\u001b[39m df_pivot\u001b[38;5;241m.\u001b[39mloc[producto_ejemplo]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 21\u001b[0m prediccion \u001b[38;5;241m=\u001b[39m \u001b[43mpredecir_t2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducto_ejemplo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43multimos_12_ejemplo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicción t+2 para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproducto_ejemplo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediccion[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: predecir_t2() missing 2 required positional arguments: 'mes' and 'quarter'"
     ]
    }
   ],
   "source": [
    "def predecir_t2(product_id, ultimos_12_meses,mes, quarter):\n",
    "    \"\"\"\n",
    "    product_id: ID del producto (ej: 'P001').\n",
    "    ultimos_12_meses: Array con las tn de los últimos 12 meses (en orden cronológico).\n",
    "    \"\"\"\n",
    "    scaler = scalers[product_id]\n",
    "    ultimos_12_scaled = scaler.transform(np.array(ultimos_12_meses).reshape(-1, 1))\n",
    "    mes_scaled = (mes - 1) / 11  # Normalizado a [0, 1]\n",
    "    quarter_scaled = (quarter - 1) / 3  # quarter ∈ [0, 3]\n",
    "    ultimos_12_scaled = np.concatenate([\n",
    "            ultimos_12_scaled.reshape(-1, 1),\n",
    "            np.array([[mes_scaled], [quarter_scaled]])\n",
    "        ], axis=0)\n",
    "    X_new = ultimos_12_scaled.reshape(1, 14, 1)\n",
    "    y_pred_scaled = model.predict(X_new)\n",
    "    return (y_pred_scaled * scaler.scale_) + scaler.center_\n",
    "\n",
    "# Ejemplo: Predecir para el primer producto en el test set\n",
    "producto_ejemplo = 20001\n",
    "ultimos_12_ejemplo = df_pivot.loc[producto_ejemplo].iloc[-12:].values\n",
    "prediccion = predecir_t2(producto_ejemplo, ultimos_12_ejemplo)\n",
    "print(f\"Predicción t+2 para {producto_ejemplo}: {prediccion[0][0]:.2f} tn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435d86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "os.makedirs(\"modelos\", exist_ok=True)\n",
    "# Guardar el modelo (formato HDF5)\n",
    "model.save(f'modelos/{study_name}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176d9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar pesos\n",
    "model.save_weights(f'modelos/pesos_{study_name}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e8b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 759ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "product_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "prediccion_t2",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "492d908d-2e38-43c0-ae69-35fa9e7e99dc",
       "rows": [
        [
         "0",
         "20001",
         "1361.2875192726735"
        ],
        [
         "1",
         "20002",
         "1043.8580510547283"
        ],
        [
         "2",
         "20003",
         "788.9439685025851"
        ],
        [
         "3",
         "20004",
         "548.2010769000863"
        ],
        [
         "4",
         "20005",
         "459.58012592493594"
        ],
        [
         "5",
         "20006",
         "461.43421423503474"
        ],
        [
         "6",
         "20007",
         "519.2550825915939"
        ],
        [
         "7",
         "20008",
         "472.4823374916517"
        ],
        [
         "8",
         "20009",
         "501.111559642937"
        ],
        [
         "9",
         "20010",
         "419.1990246578755"
        ],
        [
         "10",
         "20011",
         "356.94173946283627"
        ],
        [
         "11",
         "20012",
         "399.84011566753674"
        ],
        [
         "12",
         "20013",
         "419.4515652169454"
        ],
        [
         "13",
         "20014",
         "428.00844405487925"
        ],
        [
         "14",
         "20015",
         "395.41079896429255"
        ],
        [
         "15",
         "20016",
         "411.9635375478163"
        ],
        [
         "16",
         "20017",
         "293.739708700138"
        ],
        [
         "17",
         "20018",
         "277.89165918423765"
        ],
        [
         "18",
         "20019",
         "344.5588589517953"
        ],
        [
         "19",
         "20020",
         "271.50335381246623"
        ],
        [
         "20",
         "20021",
         "257.31780531894884"
        ],
        [
         "21",
         "20022",
         "277.80511461076685"
        ],
        [
         "22",
         "20023",
         "240.32180299309013"
        ],
        [
         "23",
         "20024",
         "236.27101965097862"
        ],
        [
         "24",
         "20025",
         "232.5451515131602"
        ],
        [
         "25",
         "20026",
         "244.89742980266885"
        ],
        [
         "26",
         "20027",
         "225.82454945910715"
        ],
        [
         "27",
         "20028",
         "236.95617558670835"
        ],
        [
         "28",
         "20029",
         "176.5665030377747"
        ],
        [
         "29",
         "20030",
         "235.47762182590964"
        ],
        [
         "30",
         "20031",
         "197.1303851215578"
        ],
        [
         "31",
         "20032",
         "253.91794901001157"
        ],
        [
         "32",
         "20033",
         "204.13399233746543"
        ],
        [
         "33",
         "20035",
         "181.997611599911"
        ],
        [
         "34",
         "20037",
         "126.39095400310843"
        ],
        [
         "35",
         "20038",
         "171.0240997832238"
        ],
        [
         "36",
         "20039",
         "169.49133328176052"
        ],
        [
         "37",
         "20041",
         "139.18757849208288"
        ],
        [
         "38",
         "20042",
         "154.0539316054138"
        ],
        [
         "39",
         "20043",
         "164.2889855806089"
        ],
        [
         "40",
         "20044",
         "156.24761800536245"
        ],
        [
         "41",
         "20045",
         "149.51476407368844"
        ],
        [
         "42",
         "20046",
         "134.64230063148878"
        ],
        [
         "43",
         "20047",
         "159.01364842678083"
        ],
        [
         "44",
         "20049",
         "190.27994639442497"
        ],
        [
         "45",
         "20050",
         "144.26095052543297"
        ],
        [
         "46",
         "20051",
         "143.36966716351674"
        ],
        [
         "47",
         "20052",
         "158.67104631944105"
        ],
        [
         "48",
         "20053",
         "146.2618922515887"
        ],
        [
         "49",
         "20054",
         "133.28788163701824"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 780
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>prediccion_t2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>1361.287519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>1043.858051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>788.943969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>548.201077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>459.580126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>21263</td>\n",
       "      <td>0.009634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>21265</td>\n",
       "      <td>0.016413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>21266</td>\n",
       "      <td>0.017833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>21267</td>\n",
       "      <td>0.014523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>21276</td>\n",
       "      <td>0.005206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id  prediccion_t2\n",
       "0         20001    1361.287519\n",
       "1         20002    1043.858051\n",
       "2         20003     788.943969\n",
       "3         20004     548.201077\n",
       "4         20005     459.580126\n",
       "..          ...            ...\n",
       "775       21263       0.009634\n",
       "776       21265       0.016413\n",
       "777       21266       0.017833\n",
       "778       21267       0.014523\n",
       "779       21276       0.005206\n",
       "\n",
       "[780 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "productos_ok = pd.read_csv(\"https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/product_id_apredecir201912.txt\", sep=\"\\t\")\n",
    "productos_ok = productos_ok[\"product_id\"].unique()\n",
    "def predecir_todos(productos_a_predecir):\n",
    "    predicciones = {}\n",
    "    for producto in productos_a_predecir:\n",
    "        ultimos_12 = df_pivot.loc[producto].iloc[-12:].values\n",
    "        mes = pd.to_datetime(df_pivot.columns[-1], format=\"%Y%m\").month\n",
    "        quarter = pd.to_datetime(df_pivot.columns[-1], format=\"%Y%m\").quarter\n",
    "        prediccion = predecir_t2(producto, ultimos_12,mes, quarter)\n",
    "        predicciones[producto] = prediccion[0][0]\n",
    "    return predicciones\n",
    "predicciones = predecir_todos(productos_ok)\n",
    "predicciones_df = pd.DataFrame.from_dict(predicciones, orient='index', columns=['prediccion_t2'])\n",
    "predicciones_df.index.name = 'product_id'\n",
    "predicciones_df.reset_index(inplace=True)\n",
    "predicciones_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f97b5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_df.columns = [\"product_id\", \"tn\"]\n",
    "predicciones_df.to_csv(f\"../../results/{study_name}.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318a2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
