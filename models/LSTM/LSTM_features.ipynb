{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6cbc4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de la matriz: (1233, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../datasets/sell-in.txt.gz\", sep=\"\\t\")\n",
    "df = df.groupby(by=[\"periodo\",\"product_id\"]).agg({\"tn\":\"sum\"}).reset_index()\n",
    "df[\"periodo\"] = pd.to_datetime(df[\"periodo\"], format=\"%Y%m\")\n",
    "df_pivot = df.pivot(index=\"product_id\", columns=\"periodo\", values=\"tn\").fillna(0)\n",
    "print(f\"Shape de la matriz: {df_pivot.shape}\")  # (800 productos x 36 meses)\n",
    "del df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f669b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_with_lags_as_features(data, window_size=12, horizon=2, lags=[1, 3, 6]):\n",
    "    \"\"\"\n",
    "    Crea ventanas donde cada timestep tiene como features: el valor original y sus lags\n",
    "    \"\"\"\n",
    "    total_length = len(data)\n",
    "    start_idx = window_size + max(lags) + horizon - 1\n",
    "    n_samples = total_length - start_idx - (horizon - 1)\n",
    "\n",
    "    n_features = 1 + len(lags)  # valor actual + lags\n",
    "    X = np.zeros((n_samples, window_size, n_features))\n",
    "    y = np.zeros((n_samples, 1))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        current_idx = start_idx + i\n",
    "        for t in range(window_size):\n",
    "            base_idx = current_idx - (window_size - t)\n",
    "            features = [data[base_idx]]  # valor actual\n",
    "            for lag in lags:\n",
    "                features.append(data[base_idx - lag])\n",
    "            X[i, t] = features\n",
    "\n",
    "        y[i] = data[current_idx + horizon - 1]\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c432ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En lugar de normalizar producto por producto (muy lento), hazlo vectorizado\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Guardar los scalers para cada producto\n",
    "scalers = {producto: StandardScaler() for producto in df_pivot.index}\n",
    "scaled_data = np.zeros_like(df_pivot.values)\n",
    "\n",
    "# Normalización vectorizada\n",
    "for i, producto in enumerate(df_pivot.index):\n",
    "    scaled_data[i] = scalers[producto].fit_transform(df_pivot.loc[producto].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6799318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar numpy para crear ventanas de manera vectorizada\n",
    "def create_windows(data, window_size=12, horizon=2):\n",
    "    n_samples = data.shape[0] - window_size - horizon + 1\n",
    "    X = np.zeros((n_samples, window_size, 1))\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        X[i] = data[i:i+window_size].reshape(-1, 1)\n",
    "        y[i] = data[i+window_size+horizon-1]  # t+2\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "X, y, productos = [], [], []\n",
    "\n",
    "lags = [1, 6, 12]\n",
    "\n",
    "\n",
    "\n",
    "for i_producto, producto in enumerate(df_pivot.index):\n",
    "    serie = scaled_data[i_producto]\n",
    "    X_prod, y_prod = create_windows_with_lags_as_features(serie, lags=lags)\n",
    "\n",
    "    X.append(X_prod)\n",
    "    y.append(y_prod)\n",
    "    productos.extend([producto] * len(X_prod))\n",
    "\n",
    "X = np.vstack(X)\n",
    "y = np.vstack(y)\n",
    "\n",
    "productos_unicos = df_pivot.index.unique()\n",
    "productos_train, productos_test = train_test_split(productos_unicos, test_size=0.2, random_state=42)\n",
    "\n",
    "# Máscaras para filtrar\n",
    "train_mask = [p in productos_train for p in productos]\n",
    "test_mask = [p in productos_test for p in productos]\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f77aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "39/39 [==============================] - 6s 51ms/step - loss: 1.0842 - mae: 0.7733 - val_loss: 1.1548 - val_mae: 0.8154 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.8437 - mae: 0.6705 - val_loss: 1.1593 - val_mae: 0.8125 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.7922 - mae: 0.6447 - val_loss: 1.1696 - val_mae: 0.8102 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7738 - mae: 0.6321 - val_loss: 1.1118 - val_mae: 0.7858 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.7429 - mae: 0.6177 - val_loss: 1.0877 - val_mae: 0.7601 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7363 - mae: 0.6103 - val_loss: 1.0383 - val_mae: 0.7353 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7207 - mae: 0.6055 - val_loss: 0.9660 - val_mae: 0.6914 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.7123 - mae: 0.6018 - val_loss: 0.8896 - val_mae: 0.6637 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6986 - mae: 0.5893 - val_loss: 0.9089 - val_mae: 0.6568 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.7075 - mae: 0.5943 - val_loss: 0.7963 - val_mae: 0.6226 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.6961 - mae: 0.5896 - val_loss: 0.7402 - val_mae: 0.6013 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6883 - mae: 0.5869 - val_loss: 0.7396 - val_mae: 0.6001 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.6804 - mae: 0.5811 - val_loss: 0.7066 - val_mae: 0.6055 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6711 - mae: 0.5755 - val_loss: 0.6854 - val_mae: 0.5835 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6709 - mae: 0.5741 - val_loss: 0.6815 - val_mae: 0.5607 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6584 - mae: 0.5685 - val_loss: 0.6723 - val_mae: 0.5686 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.6554 - mae: 0.5619 - val_loss: 0.6523 - val_mae: 0.5465 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6549 - mae: 0.5651 - val_loss: 0.6555 - val_mae: 0.5335 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.6735 - mae: 0.5732 - val_loss: 0.6754 - val_mae: 0.5280 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6417 - mae: 0.5551 - val_loss: 0.6542 - val_mae: 0.5516 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.6446 - mae: 0.5609 - val_loss: 0.6663 - val_mae: 0.5425 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.6436 - mae: 0.5559 - val_loss: 0.6598 - val_mae: 0.5508 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.6346 - mae: 0.5538 - val_loss: 0.6474 - val_mae: 0.5432 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6233 - mae: 0.5483 - val_loss: 0.6426 - val_mae: 0.5365 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6185 - mae: 0.5458 - val_loss: 0.6385 - val_mae: 0.5320 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6181 - mae: 0.5462 - val_loss: 0.6398 - val_mae: 0.5287 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6196 - mae: 0.5436 - val_loss: 0.6409 - val_mae: 0.5295 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6202 - mae: 0.5444 - val_loss: 0.6402 - val_mae: 0.5340 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6205 - mae: 0.5446 - val_loss: 0.6408 - val_mae: 0.5305 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6178 - mae: 0.5453 - val_loss: 0.6404 - val_mae: 0.5318 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6138 - mae: 0.5417 - val_loss: 0.6406 - val_mae: 0.5315 - lr: 1.0000e-05\n",
      "Epoch 32/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6200 - mae: 0.5443 - val_loss: 0.6408 - val_mae: 0.5307 - lr: 1.0000e-05\n",
      "Epoch 33/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.6157 - mae: 0.5422 - val_loss: 0.6410 - val_mae: 0.5306 - lr: 1.0000e-05\n",
      "Epoch 34/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6178 - mae: 0.5451 - val_loss: 0.6413 - val_mae: 0.5299 - lr: 1.0000e-05\n",
      "Epoch 35/200\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.6161 - mae: 0.5435 - val_loss: 0.6414 - val_mae: 0.5292 - lr: 1.0000e-05\n",
      "Epoch 36/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6193 - mae: 0.5448 - val_loss: 0.6413 - val_mae: 0.5293 - lr: 1.0000e-06\n",
      "Epoch 37/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6162 - mae: 0.5432 - val_loss: 0.6413 - val_mae: 0.5291 - lr: 1.0000e-06\n",
      "Epoch 38/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6124 - mae: 0.5408 - val_loss: 0.6413 - val_mae: 0.5291 - lr: 1.0000e-06\n",
      "Epoch 39/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6213 - mae: 0.5465 - val_loss: 0.6412 - val_mae: 0.5290 - lr: 1.0000e-06\n",
      "Epoch 40/200\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.6164 - mae: 0.5407 - val_loss: 0.6412 - val_mae: 0.5291 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "# Arquitectura mejorada\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='tanh', input_shape=(12, 4), return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Optimizador con learning rate ajustable\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Callbacks para mejorar el entrenamiento\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5)\n",
    "]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Entrenamiento con validación\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,  # Más epochs pero con early stopping\n",
    "    batch_size=256,  # Tamaño de batch ajustado\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905852da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_t2(product_id, ultimos_12_meses, lags=[1, 3, 6]):\n",
    "    \"\"\"\n",
    "    Predice usando 12 pasos y lags como features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scaler = scalers[product_id]\n",
    "        # Escalamos la serie completa primero\n",
    "        serie = np.array(ultimos_12_meses, dtype=np.float32).reshape(-1, 1)\n",
    "        serie_scaled = scaler.transform(serie).flatten()\n",
    "\n",
    "        X_new = []\n",
    "\n",
    "        # Generar 12 pasos con sus features\n",
    "        for i in range(12):\n",
    "            idx = i\n",
    "            base = serie_scaled[idx]\n",
    "            features = [base]\n",
    "            for lag in lags:\n",
    "                if idx - lag >= 0:\n",
    "                    features.append(serie_scaled[idx - lag])\n",
    "                else:\n",
    "                    # Si no hay suficiente historial para el lag, rellenar con 0\n",
    "                    features.append(0.0)\n",
    "            X_new.append(features)\n",
    "\n",
    "        X_new = np.array(X_new).reshape(1, 12, 1 + len(lags))\n",
    "        y_pred_scaled = model.predict(X_new, verbose=0)\n",
    "        y_pred = (y_pred_scaled * scaler.scale_) + scaler.mean_\n",
    "\n",
    "        return y_pred\n",
    "    except KeyError:\n",
    "        print(f\"Producto {product_id} no encontrado en los scalers\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al predecir para {product_id}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcce96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_todos(productos_a_predecir, n_workers=4):\n",
    "    \"\"\"\n",
    "    Predice en paralelo para muchos productos.\n",
    "    \"\"\"\n",
    "    def predict_single(producto):\n",
    "        try:\n",
    "            ultimos_12 = df_pivot.loc[producto].iloc[-12:].values\n",
    "            pred = predecir_t2(producto, ultimos_12)\n",
    "            if pred is not None:\n",
    "                return producto, pred[0][0]\n",
    "        except:\n",
    "            pass\n",
    "        return producto, np.nan\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        results = list(executor.map(predict_single, productos_a_predecir))\n",
    "\n",
    "    return dict(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe825b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "productos_ok = pd.read_csv(\"https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/product_id_apredecir201912.txt\", sep=\"\\t\")\n",
    "productos_ok = productos_ok[\"product_id\"].unique()\n",
    "predicciones = predecir_todos(productos_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "710d4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([predicciones.keys(),predicciones.values()])\n",
    "df = df.T\n",
    "df.columns = [\"product_id\", \"tn\"]\n",
    "df[\"product_id\"] = df[\"product_id\"].astype(int)\n",
    "df.to_csv(\"../../results/LSTM_lags_1_6_12.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17367e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
